# 卒論

---

## はじめに

### 研究背景
画像のクラス分類の精度は、畳み込みニューラルネットワーク(convolutional neural network, CNN)によって、飛躍的に向上してきた。畳み込みニューラルネットワークによって飛躍的な精度の向上を達成した理由として、畳み込み層と呼ばれる特徴を抽出する層が局所的なパターンを捉え、それらを積み重ねることでより抽象度の高い特徴を抽出できるようになった点が挙げられる。

しかし、畳み込みニューラルネットワークを用いる際に発生する問題点として、トレーニング用のデータ(トレーニングデータ)とテスト用のデータ(テストデータ)でのドメインの違いによる精度の低下が挙げられる。ドメインとは、データの特性が異なる状況を指す。例えば、自然画像のデータセットで学習されたモデルでは、手書きのイラストに対しては、精度が低下してしまうことが、考えられる。

このような問題に対応する問題は、ドメイン適応として知られている。ドメイン適応のために、トレーニングデータのドメインとテストデータのドメイン間に存在する特徴表現の分布の差を小さくすることが重要であると、考えられる。

本研究では、画像のクラス分類における、ドメイン適応に取り組む。特に、トレーニングデータのドメインと比較して、テストデータのドメインのラベル付き画像が少数のみ状態における問題を扱う。ドメイン適応の手法として、トレーニングセットのデータをテストセットのドメインにスタイル転送を行うことによって、テストデータのドメインに近い特徴を持つ画像を生成する。そして、その画像を用いて、画像のクラス分類を行う畳み込みニューラルネットワークの学習を行う。この手法によって、トレーニング時とテスト時のドメイン差が緩和され、テストデータのドメインに対する画像のクラス分類の精度が向上することを期待する。実験では、複数のドメインがあるPACSデータセットを用いて、提案手法による精度の変化を測定する。

 本研究では、スタイル転送を、Leon A. Gatysによってニューラルスタイル転送(Neural Style Transfer, NST)によって提案された手法を用いて行う。この手法では、コンテンツを示す画像(コンテンツ画像)とスタイルを示す画像(スタイル画像)を1枚ずつ用いることによって、コンテンツ画像の構造を保持しつつ、スタイル画像の特徴を反映した画像を生成することができる。しかし、テストデータのドメインを示すスタイル画像は1枚とは限らず、スタイルを示す画像を複数枚用いることができる場合も想定される。そこで、スタイル画像を2枚以上用いることができるように、損失関数の拡張と、勾配計算に用いる損失関数を切り替えながら画像を更新する手法の導入を行う。



### 本論文の構成
本論文の構成は、以下のとおりである。
本章では、本研究を行うに至った問題意識と、本研究の提案する手法について述べる。また、本論文の構成について述べる。
第2章では、関連研究として、畳み込みニューラルネットワークの概要と、本研究で用いるVGGNetの構造と特徴について説明する。また、ドメイン適応およびデータ拡張に関する既存研究を整理し、ニューラルスタイル転送をデータ拡張に応用した研究事例について述べる。
第3章では、本研究で提案するニューラルスタイル転送を活用するデータ生成の手法について述べる。また、提案する手法によって期待する成果について、評価を行う手法も含めて述べる。
第4章では、本研究で行う実験の具体的な環境と、実験の結果と考察について述べる。
第5章では、結論として、本研究の実験の結果から導かれる考察と今後の課題について述べる。

## 関連研究
本章では、本研究で利用する畳み込みニューラルネットワークの概要とその特徴を示し、本研究で実際に用いるVGGNetの構造と特徴について、説明する。また、ドメイン適応とデータ拡張について、問題設定と既存の研究を説明する。さらに、ニューラルスタイル転送をデータ拡張に用いた事例を紹介する。そして、本研究の目的について述べる

### 畳み込みニューラルネットワーク
畳み込みニューラルネットワーク(convolutional neural network, CNN)とは、畳み込み層を用いているニューラルネットワークを指し、画像認識の分野で用いられている。畳み込みニューラルネットワークの特徴として、畳み込み層が画像データの一部分ずつにフィルタを適用し、データの局所的なパターンを検出することができることが、挙げられる。

以降では、畳み込み層(Convolutional Layer)、プーリング層(Pooling Layer)、活性化関数(Activation Function)、全結合層(Fully Connected Layer)およびDropoutについて、それぞれの仕組みを、説明する。

#### 畳み込み層
畳み込み層(Convolutional Layer)は、入力画像の局所的な特徴を抽出するために、機能する。畳み込み層では、フィルタ(あるいはカーネル)と呼ばれる小さな重み行列を、ストライドとよばれる移動幅に従って、位置を変えながら適用し、そのたびに、対応領域との積和演算を行う。この演算は、畳み込み演算と呼ばれる。

図は、畳み込み演算の流れを示している。

畳み込み演算を行う際、入力データの周囲に新たなピクセルを追加する、パディングという作業が行われる。パディングによって、フィルタを入力データの端まで適用できるようになる。パディングを行うことによって、出力の空間的なサイズが減少することを、抑制することができる。パディングのうち、周囲のデータをゼロで埋める手法は、ゼロパディングと呼ばれる。

図は、ゼロパディングの流れを示している図である。

#### プーリング層
プーリング層(Pooling Layer)は、畳み込み層において抽出された特徴マップを圧縮する層である。計算量を削減しつつ、位置ずれに対する頑健性を高める役割がある。プーリング層では、特徴マップを小さな領域に分割し、各領域から代表値を出力することによって、重要な特徴を保ちながら情報を圧縮する。

主なプーリング層の操作として、最大値プーリング(Max Pooling)や平均値プーリング(Average Pooling)がある。最大値プーリングは、各領域の中で最も強く反応した値を選び出すため、特徴の存在そのものを強調する働きがある。平均値プーリングは、値を均等に扱って平均を求める操作であり、領域全体の情報をなだらかにまとめるため、過度に尖った反応を抑えながら特徴を集約する傾向がある。

図は最大値プーリングの操作を示し、図は平均値プーリングの操作を示している。

#### 活性化関数
活性化関数(Activation Function)は、ニューラルネットワークにおいて各ノードが入力信号を受け取った際に、それをどのような形で出力に変換するかを決定する役割を持つ。非線形な活性化関数を導入することによって、ニューラルネットワークは非線形な関係を学習することが可能となり、複雑なデータのパターンを表現できるようになる。

もし非線形な活性化関数を用いない場合、ネットワーク全体は単純な線形変換の連続に過ぎず、層を重ねても表現力は線形モデルと同等にとどまる。そのため、複雑なパターンや特徴を捉えることができず、高度な分類や認識タスクにおいて十分な性能を発揮できない。

ここでは、活性化関数であるReLU、Sigmoid、Softmaxについて説明する。

##### ReLU
ReLUは、活性化関数の1つである。ランプ関数とも呼ばれる。式は、ReLUの関数を示している。xをReLUへの入力、ReLU(x)をReLUの出力とする。ReLUは、入力が0以下の場合は0を、0より大きい場合はそのままの値を出力する関数である。計算が単純であり、計算のコストが小さいという利点がある。入力が0であるとき不連続となり微分は定義できない、という特徴がある。

$`ReLU(x)=max(0, x)`$

図は、ReLUの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

##### Sigmoid
Sigmoidは、活性化関数の１つである。式は、Sigmoidの関数を示している。xをSigmoidへの入力、Sigmoid(x)をSigmoidの出力とする。Sigmoidは、入力値を0から1の範囲へ変換する関数である。この性質により、確率的な解釈が可能となる。入力値が大きい、または、小さい場合、勾配がほぼゼロになり、学習が進みにくくなるという問題がある。この問題は、勾配消失問題として知られている。

$`Sigmoid(x)=\frac{1}{1+e^{-x}}`$

図は、Sigmoidの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

##### Softmax
Softmaxは、活性化関数の1つである。式は、Softmaxの関数を示している。ベクトル$`z(z_{1},...z_{K})`$をSoftmaxへの入力、$`Softmax(z_{i})`$をSoftmaxの出力とする。多クラス分類の問題において用いられる活性化関数である。出力は、それぞれ0から1の間の値を取り、すべての出力の合計は1となる。そのため、入力ベクトルを各クラスに対する確率として正規化していると言える。また、Softmaxは、Sigmoidの多次元拡張であると言える。

図は、Softmaxの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

$`\mathrm{Softmax}(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}`$

#### 全結合層
全結合層(Fully Connected Layer)は、すべてのノードが接続している層である。受け取った特徴を組み合わせてより抽象的な概念を学習し、最終的な判断や出力を行う役割を持つ。畳み込み層などで抽出された特徴を最終的に分類するために、この全結合層が使われる。

図は、全結合層の接続の構造を示している。

#### Dropout
Dropoutは、正則化のための手法の1つである。Srivastavaらによって、2014年に提案された。正則化とは、モデルが過学習を防ぐための手法である。過学習とは、モデルが訓練データに過度に適合し、未知のデータに対する汎化性能が低下してしまう状態を指す。

Dropoutは、学習の過程でネットワークから、ランダムに、ノードをその接続ごと取り除く作業のことを指す。Dropoutは、隠れ層だけでなく入力層のノードにも適用されるが、出力層には適用されない。学習時には一部のノードが取り除かれるが、推論時には全てのノードを使用する。その際、出力を調整し学習時の挙動と整合性のあるようにする。

### VGGNet
VGGNetは、代表的な畳み込みニューラルネットワークの1つである。オックスフォード大学のVisual Geometry Groupによって、2014年に提案された。VGGNetは、16層(畳み込み層13層と全結合層3層)で構成されるVGG16と、19層(畳み込み層16層と全結合層3層)であるVGG19がある。VGGNetでは、3×3のフィルタを用いる畳み込み層を一貫して用い、層を深くすることによって、抽象的な特徴を捉えるようになることが、図られている。VGGが提案される以前に主流となっていたAlexnetでは、複数の異なる大きさのフィルタを用いていた。

まず、VGG16について説明する。VGG16は、畳み込み層13層と全結合層3層で構成されている。パラメータ数は約1億3800万である。図は、VGG16のアーキテクチャを示している。層の深さが比較的浅いため、一般に学習速度は速い一方で、精度はやや低くなる傾向がある。処理能力を比較的多く必要としないため、精度と計算効率のバランスをとる必要がある場合は、VGG16を選択するべきであると、考えられる。

次に、VGG19について説明する。VGG19は、畳み込み層16層と全結合層3層で構成されている。パラメータ数は約1億4400万である。図は、VGG19のアーキテクチャを示している。層が比較的深いため、一般に学習速度は遅いが、精度がやや高くなる傾向がある。比較的多くの処理能力が必要になるので、十分な計算リソースがある場合は、VGG19を選択するべきであると、考えられる。

### スタイル転送
スタイル転送とは、スタイルを示している画像(スタイル画像)とコンテンツを示している画像(コンテンツ画像)から、コンテンツ画像の内部構造を維持したままスタイルの情報を反映した、新しい画像を生成する技術である。
ニューラルスタイル転送が提案される以前、スタイル転送の手法として、Image Quiltingという手法などが提案されていた。Image Quiltingとは、画像を小さく分割し、それをつなぎ合わせることによって、全体として自然な見た目を保ったまま新たな画像を生成する手法である。その後、ニューラルスタイル転送という手法が提案され、畳み込みニューラルネットワークから得られる特徴を用いることができるようになった。さらに、敵対的生成ネットワーク(GAN, Generative Adversarial Network)を用いるスタイル転送の手法が、提案されている。GANとは、生成器と判別器の2つから構成されており、生成器が本物らしい画像を生成し、判別器がそれを本物か偽物か判別するという対立的な学習を通じて、生成の質を高めていくモデルである。GANを用いたスタイル転送では、判別器のフィードバックを通じて、生成画像がより自然で一貫したスタイル分布を持つよう学習が進められる。

ここでは、本研究でスタイル転送に用いるニューラルスタイル転送ついて説明する。

#### ニューラルスタイル転送
ニューラルスタイル転送(Neural Style Transfer, NST)とは、畳み込みニューラルネットワークが形成する特徴表現を用いて、コンテンツ画像がもつ情報とスタイル画像がもつ情報を用いて最適化することによって、新たな画像を生成する手法である。ニューラルスタイル転送には、記述的な手法と生成的な手法がある。記述的手法とは、画像のピクセルを反復的に更新する手法を指し、生成的手法は、スタイルを事前学習されたモデルを用いて、画像の生成を1回の順伝播で行う手法を指す。Leon A. Gatysらによって提案されたニューラルスタイル転送の手法と、Justin Johnsonらによって提案されたニューラルスタイル転送の手法について、説明する。

##### Leon A. Gatysらによるニューラルスタイル転送の手法
ニューラルスタイル転送は、Leon A. Gatysらによって2015年にはじめて提案された。その後、2016年に、査読付き会議で提案された。この手法は、記述的手法である。

図は、Leon A. Gatysらによって提案されたニューラル転送のアーキテクチャを示している。

この手法では、事前学習された畳み込みニューラルネットワークに入力し、特徴マップを抽出する。得られた特徴マップは$`F^{l}_{ij}`$と表す。lは層の位置を、iとjはフィルタの位置を表す。畳み込み層では、層ごとに捉えている情報が異なり、浅い層ではエッジや色のような低次特徴、深い層では物体の構造や形状のような高次特徴が得られる。この性質を利用することによって、コンテンツとスタイルの役割を分担させることが可能となる。

スタイルの特徴は、式のように、グラム行列を用いて表現される。グラム行列は、フィルターとフィルターのベクトル同士の内積で計算される。グラム行列を用いることによって、位置に依存せず、様々なスケールで情報を捉えることができる。$`N_{l}`$は層lにおけるフィルタの数を表す。$`G_{l}`$は式で示されているように、行と列の数は、それぞれフィルタの数と等しい。

$`G^{l}_{ij} = \Sigma_{k} F^{l}_{ik}F^{l}_{jk}`$

$`G^{l}\in R^{N_{l}☓N_{l}}`$

コンテンツの損失($`L_{content}`$)は式のように、スタイルの損失($`L_{style}`$)は式にように、それぞれ計算する。
$`P_{ij}^{l}`$はコンテンツ画像から得られた特徴マップを、$`A_{ij}^{l}`$はスタイル画像から得られたグラム行列の成分を表す。
また、$`M_{l}`$は層$`l`$における特徴マップの要素数を、$`w_{l}`$は層$`l`$におけるスタイル損失の重み係数を表す。
$`E_{l}`$は、は層$`l`$におけるスタイルの誤差（スタイル損失）を表す。そして、式のように、損失を合計する。$`\alpha`$と$`\beta`$は、スタイルとコンテンツの重みを表す。


$`L_{content} = \frac{1}{2}\Sigma_{i,j}(F_{ij}^{l}-P_{ij}^{l})^{2}`$

$`E_{l} = \frac{1}{4M_{l}^{2}M_{l}^{2}}\Sigma_{i,j} (G_{ij}^{l}-A_{ij}^{l})^{2}`$

$`L_{style} = \Sigma^{L}_{l=0}w_{l}E_{l}`$

$`L_{total} = \alpha L_{content} + \beta L_{style}`$

初期画像から、合計された損失($`L_{total}`$)が小さくなるように、勾配の計算をする。そして、画像を更新する。

##### Justin Johnsonらによるニューラルスタイル転送の手法
Leon A. Gatysによるニューラルスタイル転送の手法が提案された後、2016年に、Justin Johnsonらによって、生成的手法のスタイル転送の手法が提案された。この手法では、一回の順伝播によってスタイル転送を行うことができる。この手法では、画像生成のためのネットワークを一度学習することで、Leon A. Gatysらによって提案された手法よりも、高速にスタイル転送された画像を生成することができる。この手法は、生成的手法である。

図は、Justin Johnsonらによって提案されたニューラル転送のアーキテクチャを示している。

まず、この手法では、図ではImage Transform Netと示されている、画像を実際に生成するネットワークによって、入力画像から出力画像を生成する。そして、Image Transform Netによって生成された画像、スタイル画像とコンテンツ画像の3つの画像をそれぞれVGG16に入力し、特徴量を抽出する。そして、コンテンツの誤差とスタイルの誤差を、Leon A. Gatysによるニューラルスタイル転送の手法と同じように、計算する。そして、その誤差の情報を用いて、Image Transform Netの重みを更新する。これを繰り返すことで、最終的には特定のスタイルに対して最適化されたImage Transform Netを得ることができる。学習後は任意の画像を入力すると、高速にスタイル転送画像を生成できるようになる。

### ドメイン適応
学習時のデータと推論時のデータのドメインが異なる際に、一般に、推論の精度が低下することが知られている。ドメインとは、データの特性が異なる環境や条件を指す。特に、本研究におけるドメインとは、自然画像、水彩画風のイラスト、漫画風のイラストなどのように、画像の種類を指す。ドメイン適応とは、学習時のデータと推論時のデータのドメインが異なる際に、推論の精度を向上させる問題のことを指す。以下では、学習時に用いるデータのドメインのことをソースドメイン、推論時に用いるデータのドメインのことをターゲットドメインという。

ドメイン適応の問題設定として、教師ありドメイン適応、半教師ありドメイン適応、教師なしドメイン適応の3つが挙げられる。教師ありドメイン適応とは、学習時に用いるソースドメインだけでなく、推論対象となるターゲットドメインについても、ラベル付きデータが利用可能であることを想定した状況である。半教師ありドメイン適応では、ソースドメインにはラベル付きデータが存在するが、ターゲットドメインには少数のラベル付きデータのみが利用可能であることを想定した状況である。教師なしドメイン適応とは、ソースドメインにはラベル付きデータが存在する一方で、ターゲットドメインにはラベルなしデータのみが存在することを想定した状況である。

ドメイン適応のための手法として、ソースドメインとターゲットドメインの特徴分布の差異が性能低下の原因であることから、敵対的学習による方法や、統計量を調整する方法、また、生成モデルを用いる方法が提案されてきた。
敵対的学習によって、ソースドメインとターゲットドメインを識別しようとする識別器と、それを欺くように特徴を生成する特徴抽出器を同時に学習することで、両ドメインの特徴表現が区別できなくなり、結果としてドメイン間の差異を抑えた特徴空間を獲得することができる。
また、統計量を調整することによって、ソースドメインとターゲットドメインにおける特徴量の平均や分散などの統計的性質を揃えることで、分布のずれを直接的に低減し、ドメインの違いに依存しない安定した推論性能を実現することができる。
さらに、生成モデルを使ったアプローチでは、ターゲットドメインに似たデータを生成することで、ソースドメインのラベル付きデータをターゲットドメイン向けに変換することができる。


### データ拡張
データ拡張とは、訓練データが少ない際に、データを増やすことによって、モデルの精度を向上することを目指す手法のことである。一般に、データ拡張の手法は、幾何学変換と測光変換の2つのカテゴリーの分類されてきた。幾何学変換とは、サイズの変更や回転など、元の画像の形状を変更する手法を指す。一方、測光変換とは、彩度の調整やグレースケール処理など、画像のRGBを変更する手法を指す。さらに、近年では、生成的な手法によって画像を増やす手法も提案されている。

データ拡張の問題点として、不適切な方法で行うと、モデルの精度が低下してしまう、という点がある。例えば、自動車のデータセットに対して上下を反転することによるデータ拡張を行うと、実際の走行環境ではほとんど観測されない「逆さまの自動車」という不自然なデータが学習に含まれてしまい、モデルが本来学習すべき特徴と無関係なパターンを覚えてしまう可能性がある。その結果、現実の画像に対する識別性能が低下してしまう可能性が、考えられる。

### ニューラルスタイル転送を用いたデータ拡張による精度向上
先行研究として、ニューラルスタイル転送を用いてデータ拡張を行い、モデルの精度向上を図った、STaDA(Style Transfer as Data Augmentation)という研究がある。この研究では、1回の順伝播で画像を作成することができる、生成型のスタイル転送を用いている。

この研究では、以下に示している8枚の写真を、スタイル画像として、利用している。もとのデータセットに対して、スタイル画像を用いた画像生成を行うことによって、データセットの画像数をに2倍に拡張している。画像は、101個のクラスで構成されている、Caltech 101というラベル付き画像データセットを用いている。

この研究では、スタイル画像のうち、Snowを用いたスタイル転送によって生成した画像を用いることによって、精度が向上することが示された。しかし、Your nameなど、一部のスタイル画像における例では、精度の低下が示された。スタイル画像によって、精度が向上する場合と低下する場合があることが、この研究の結果からわかる。

また、この研究では、伝統的なデータ拡張手法として、FlippingとRotationを使用した実験も行っている。Flippingとは、左右を反転させた画像を生成する操作である。Rotationとは、上下を反転させた画像を生成する操作である。この研究では、Flipping単体では精度は低下するが、WaveとFlippingを組み合わせた手法では精度が向上している。データ拡張手法の組み合わせ方によって、精度に違いがあることが、この研究の結果からわかる。

### 本研究の目的
本研究では、画像のクラス分類における、半教師ありドメイン適応の問題に取り組む。半教師ありドメイン適応では、ソースドメインには十分なラベル付きデータが存在する一方で、ターゲットドメインには少数のラベル付きデータしか存在しない。そのため、ターゲットドメインの特徴を十分に学習できず、分類精度が低下するという課題がある。そこで、Leon A. Gatysらが提案したニューラルスタイル転送の手法によって、ターゲットドメインの画像をソースドメインに変換する。
一般に、ニューラルスタイル転送では、スタイル画像は1枚のみ用いる。しかし、本研究で行う問題では、スタイル画像を複数用いることができる状況を考える。このような問題設定において、それらのスタイル情報をどのように統合し、画像生成に反映させるかが課題となる。
また、Leon A. Gatysらによるニューラルスタイル転送では、初期画像として、コンテンツ画像、スタイル画像、あるいはホワイトノイズを用いることが一般的である。

そこで、本研究では、複数のスタイル画像を用いたニューラルスタイル転送におけるスタイル画像の与え方の違い、ニューラルスタイル転送における初期画像の違い、また、データ拡張手法を組み合わせることが分類精度に与える影響を調べることを目的とする。

## 提案手法
本研究の目的は、複数のスタイル画像を用いたニューラルスタイル転送の方法や初期画像の違い、また、データ拡張手法の組み合わせが分類精度に与える影響を明らかにすることである。そこで、本章では、その目的を実現するために提案する、ニューラルスタイル転送を用いたデータ生成による半教師ありドメイン適応の手法について述べる。

### 基本方針
本研究では、トレーニングセットとテストデータが異なるドメインに属しており、特にテストデータの方はラベル付き画像が少ない半教師あり学習の仮定する。そこで、複数のドメインで構成されているラベル付き画像データセットを用いる。

トレーニングセットの画像を、テストセットの画像を用いてスタイル転送することで、テストデータのドメインを反映した新たな学習用画像を生成する。スタイル転送の手法として、Leon A. Gatysらが提案したニューラルスタイル転送の手法を用いる。

Leon A. Gatysらが提案したニューラルスタイル転送では、一般に、スタイル画像を1枚のみ用いる。しかし、この研究の問題設定では、ターゲットドメインを示すスタイル画像が複数存在する状況を想定する。そこで、本研究では、複数のスタイル画像を用いたスタイル転送を可能とするために、損失関数の拡張と、勾配計算に用いる損失関数を切り替えながら画像を更新する手法を提案する。

スタイル損失の拡張は、式のように行う。

$`L_{style\_loss\_multiple}`$

そして、全体のスタイル損失は、式のようになる。

$`L_{total}=`$

そして、画像のクラス分類タスクを用いて、提案手法の効果を測定する。提

### 提案手法から期待する効果
案手法を用いることによって、ターゲットドメインに対する分類精度の向上が期待される。

## 実験
本章では、実験の目的、実験の基本構成、実験用いるデータセット、実験環境について説明する。また、本研究で行う4つの実験について、それぞれ説明する。

### 実験の目的
実験では、第3章で提案した手法を実行し、手法の有効性を確認する。また、実験の条件を変更することによる精度の変化を確認する。


### 実験の基本構成
図は、実験の流れを示している。


### データセット
本節では、本研究で用いる画像分類におけるドメイン適応の評価のために用いるPACSデータセット、畳み込みニューラルネットワークの事前学習のためにImageNetの説明を行う。

#### PACSデータセット
本研究では、ドメイン適応の精度を評価するために、4つのドメインで構成されている、PACSデータセットを用いる。Photoのドメインの画像は多くあり、それ以外のドメインの画像は少数のみある、という状況における課題を、今回の研究では扱う。PACSは、Photo、Art、Cartoon、Sketchの略であり、これら4つのドメインで構成されている、ラベル付き画像データセットである。Photoは1670枚、Artは2048枚、Cartoonは2344枚、Sketchは3929枚で構成されている。各ドメインは、dog、elephant、giraffe、guitar、horse、house、personの、7つのクラスで構成されている。本研究では、各ドメインの画像を三対一の割合で分割し、それぞれを学習用と評価用として利用する。

図は、各ドメインの、クラスごとの画像データの例である。

表は、各ドメインの、クラスごとの画像の数を示している。

#### ImageNet
また、VGGNetの事前学習を行うためにImageNetを用いる。ImageNetは、Fei-Fei Liらによって2009年に発表されたラベル付き画像データセットである。1000クラスで構成され、100万枚を超える様々な画像が含まれている。ImageNetによる学習によって獲得する特徴表現は、画像を入力とする多様なタスクに対して、普遍的に有効であることがわかっているので、転移学習に活用されている。

図は、ImageNetに含まれている画像データの例である。


### 実験環境
実験環境として、GPUを搭載している計算機を用いる。図は、実験で実際に用いる計算機のGPU、CPU、メモリ、OSを表す。GPUは、並列の数値計算を高速に行うことができる。ディープラーニングでは並列の計算を多く行うため、GPUを用いることによって、高速化を達成することができる。

ニューラルスタイル転送と画像分類の畳み込みニューラルネットワークは、Pythonを用いた。また、ニューラルネットワークの構築と学習用のライブラリとして、TensorflowとKerasを用いて実装を行った。Pythonはバージョン3.12.2、Tensorflowはバージョン2.18.1、Kerasはバージョン3.6.0を用いた。

また、ニューラルスタイル転送と画像分類の畳み込みニューラルネットワークにおいて、事前にImagenetを用いて事前学習を行っているVGGNetの重みは、Kerasによって提供されているものを用いる。入手は、VGG16およびVGG19において、以下のコードを用いてそれぞれ行った。modelという変数に、VGGのmodelが代入される。

VGG16
```
import keras
model = keras.applications.VGG16(
    weights="imagenet",
    include_top=False,
)
```

VGG19
```
import keras
model = keras.applications.VGG19(
    weights="imagenet",
    include_top=False,
)
```


本研究で画像分類を行う畳み込みニューラルネットワークは、VGG16を基盤とし、ImageNetで事前学習された重みを用いた畳み込み層の部分を特徴抽出器として利用し、そのパラメータを固定した上で、新たに全結合層および分類層を付加したものとする。図は実際のネットワークの構成を示している。

本研究で行う実験は、特に後述しない限り、ハイパーパラメータは、表に示したものを用いる。

クラス分類の精度は、Top1精度、すなわち、最も最も確率の高いクラスが正解クラスである場合に正解であるとみなす指標で、評価を行う。

### 実験1: ベースとなるクラス分類の精度
#### 目的
実験1では、ニューラルスタイル転送を用いて生成した画像を用いた場合の精度を測定し、手法の効果を確認する。また、ニューラルスタイル転送による画像の生成に要する時間と、画像のクラス認識に要する時間をそれぞれ測定し、この手法の実効性を確認する。

#### 方法
本研究では、スタイルを示す画像として、表に示した画像を用いた。図は、実際に用いるスタイル画像を示している。

#### 結果
以下の表は、画像認識の精度の結果を示している。

また、以下の表は、ニューラルスタイル転送によって画像を生成するのに要した時間を示している。1つのドメインあたり1333枚の画像を生成しており、それに要した合計の時間を示している。また、「1枚あたりの時間」の列は、合計の時間を1333で割り、1枚あたりに要した時間の平均を取ったものである。

さらに、以下の表は、画像のクラス分類のための畳み込みニューラルネットワークの学習に要した時間を示している。

ニューラルスタイル転送によってデータセットを作成するために、おおよそ1時間程度学習に要することがわかる。そして、画像のクラス分類のための畳み込みニューラルネットワークの学習に要する時間は、およそn分であることがわかる。
#### 考察
まず、クラス分類の精度の結果から、

また、ニューラルスタイル転送によってデータセットを作成するために要する時間と、画像のクラス分類のための畳み込みニューラルネットワークの学習に要する時間の結果から、本研究の提案手法では殆どの時間が、ニューラルスタイル転送に充てられている。だから、ニューラルスタイル転送の高速化が進むことによって、本研究の提案手法の実効性が高まると、考える。


### 実験2: スタイル画像を複数にした場合の精度
#### 目的
実験2では、ニューラルスタイル転送でのスタイル画像を複数枚用いることよって、精度がどのように変化するかを測定する。
スタイル画像のより精度を高めることが出来る使用方法を探る。
複数枚のスタイル画像を用いる方法は、提案手法の章で説明したように、勾配計算をする際に用いる損失関数を毎回同じにする方法と、損失関数を適切に切り替えながら勾配計算を行う方法の2つが考えられる。
ここでは、それらの方法をそれぞれ用いることによって画像を生成し、それぞれの手法によるクラス分類の精度を測定し、

#### 方法
本研究では、スタイルを示す画像として、表に示した画像を用いた。図は、実際に用いるスタイル画像を示している。
#### 結果
表はターゲットドメインがart、表はターゲットドメインがcartoon、表はターゲットドメインがsketch、における画像認識の精度の結果をそれぞれ示している。

#### 考察


### 実験3: 初期画像別の精度
#### 目的
実験3では、ニューラルスタイル転送を用いて画像を生成する歳の初期画像によって、精度がどのように変化するかを測定する。ニューラルスタイル転送では、初期画像を更新することによって、画像の生成を行う。ここで、初期画像をコンテンツ画像にする場合、スタイル画像にする場合、ホワイトノイズにする場合によって、生成される画像の質が変わると考えられる。そこで、それぞれ初期画像を使った場合の精度を比較し、最適な初期画像の選定に関する知見を得ることを目指す。

#### 方法
本研究では、スタイルを示す画像として、表に示した画像を用いた。図は、実際に用いるスタイル画像を示している。
#### 結果
表はターゲットドメインがart、表はターゲットドメインがcartoon、表はターゲットドメインがsketch、における画像認識の精度の結果をそれぞれ示している。

#### 考察


### 実験4: 古典的データ拡張手法の組み合わせた場合の精度
#### 目的
実験4では、古典的なデータ拡張の手法であるFlippingとRotationを用いた場合の精度を測定する。

#### 方法
本研究では、スタイルを示す画像として、表に示した画像を用いた。図は、実際に用いるスタイル画像を示している。
#### 結果
表はターゲットドメインがart、表はターゲットドメインがcartoon、表はターゲットドメインがsketch、における画像認識の精度の結果をそれぞれ示している。
#### 考察


### 4つの実験結果を通しての考察

## 結論
### 提案手法による成果
本研究の結果から、ニューラルスタイル転送によって画像を生成し、その画像も含めて画像のクラス分類のモデルを学習することによって、精度の向上が期待できることが、示された。

### 今後の課題と活用の○○


今後の課題として、まず、提案手法の有効性を高めるために、より高速なスタイル転送

また、画像のクラス分類以外のタスクにおける精度の変化を調べる必要があると、考える。例えば、画像のピクセルに対してラベル付けを行うセマンティックセグメンテーションや、物体の位置と範囲を推論する物体検出が挙げられる。

スタイル転送を用いることによって、ターゲットとするドメインのデータ量の不足を補うことができる可能性が考えられる具体的な例として、医療診断とリモートセンシングが挙げられると、考える。
医療診断では、例えば。
リモートセンシングでは、例えば。

---
---


# メモ

- https://www.ite.or.jp/contents/keywords/2303keyword.pdf
- https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
- https://keras.io/api/applications/vgg/
- https://www.ibm.com/jp-ja/think/topics/data-augmentation
- https://www.jstage.jst.go.jp/article/iieej/37/6/37_6_815/_pdf/-char/ja
- https://people.eecs.berkeley.edu/~efros/research/quilting/quilting.pdf
- https://speakerdeck.com/ringa_hyj/shen-ceng-xue-xi-wotukatutahua-xiang-sutairubian-huan-falsehua-tojin-madefalseli-shi?slide=2
- https://medium.com/@sandhrabijoy/vgg16-vs-vgg19-a-detailed-comparison-of-the-popular-cnn-architectures-cae5ba404352
- https://zenn.dev/monchy1017/articles/0c725f1d981881 

---
---

#表
