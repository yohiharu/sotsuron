# 卒論

---


## はじめに

### 研究背景
### 本論文の構成
本論文の構成は、以下のとおりである。
題2章では、関連研究として、畳み込みニューラルネットワークの特徴や、本研究で用いるVGGNetの構造について説明する。また、ドメイン適応およびデータ拡張に関する既存研究を整理し、ニューラルスタイル転送をデータ拡張に応用した研究事例について述べる。
題3章では、本研究で提案するニューラルスタイル転送を活用するデータ拡張の手法について述べる。また、提案する手法によって期待する成果について、評価を行う手法についても含めて述べる。
題4章では、本研究で行う実験の具体的な環境と、実験の結果と考察について述べる。
題5章では、結論として、本研究の実験の結果から導かれる考察と今後の課題について述べる。

## 関連研究
本章では、提案手法で利用する畳み込みニューラルネットワークの概要とその特徴を示し、本研究で採用するVGGNetの構造と特徴について、説明する。また、ドメイン適応とデータ拡張について、既存の研究を説明する。さらに、スタイル転送をデータ拡張に用いた事例を紹介する。

### 畳み込みニューラルネットワーク
畳み込みニューラルネットワーク(convolutional neural network, CNN)とは、畳み込みを用いているニューラルネットワークを指し、画像認識の分野で用いられている。畳み込みニューラルネットワークの特徴として、データの局所的なパターンを検出することができる。

図は、ニューラルネットワークのイメージを示している。

以降では、畳み込み層(Convolutional Layer)、プーリング層(Pooling Layer)、活性化関数、全結合層(Fully Connected Layer)およびDropoutについて、それぞれの仕組みを、説明する。

#### 畳み込み層
畳み込み層(Convolutional Layer)は、入力画像の局所的な特徴を抽出するために、機能する。

フィルタ(あるいはカーネル)と呼ばれる小さな重み行列を、ストライドとよばれる移動幅に従って、位置を変えながら適用し、そのたびに、対応領域との積和演算を行う。この演算は、畳み込み演算と呼ばれる。
図は、畳み込み演算の流れを示している図である。

畳み込みを適用する際に、入力データの周囲に新たなピクセルを追加する、パディングという作業が行われる。パディングによって、フィルタが入力データの端まで適用できるようになる。パディングを行うことによって、出力の空間的なサイズが減少することを、抑制することができる。パディングのうち、周囲のデータをゼロで埋める手法は、ゼロパディングと呼ばれる。
図は、ゼロパディングの作業の流れを示している図である。

#### プーリング層
プーリング層(Pooling Layer)は、畳み込み層において抽出された特徴マップを圧縮する層である。計算量を削減しつつ、位置ずれに対する頑健性を高める役割がある。特徴マップを小さな領域に分割し、各領域から代表値を出力することによって、重要な特徴を保ちながら情報を圧縮する。主なプーリング層の操作として、最大値プーリング(Max Poolig)や平均プーリング(Average Pooling)がある。最大値プーリングは、各領域の中で最も強く反応した値を選び出すため、特徴の存在そのものを強調する働きがある。平均プーリングは、値を均等に扱って平均を求める操作であり、領域全体の情報をなだらかにまとめるため、過度に尖った反応を抑えながら特徴を集約する傾向がある。
図は、最大値プーリング、平均値プーリングのそれぞれの操作を示している図である。

#### 活性化関数
活性化関数は、ニューラルネットワークの入力から、各ニューロンが受け取った入力信号をどのように出力するかを決定する役割を持つ。活性化関数を導入することによって、ネットワークは非線形な関係を学習することが可能となり、複雑なデータのパターンを表現できるようになる。もし非線形な活性化関数を用いない場合、ネットワーク全体は単純な線形変換の連続に過ぎず、層を重ねても表現力は線形モデルと同等にとどまる。そのため、複雑なパターンや特徴を捉えることができず、高度な分類や認識タスクにおいて十分な性能を発揮できない。

##### Relu
Reluは、活性化関数の1つである。ランプ関数とも呼ばれる。式は、Reluの関数を示している。xをReluへの入力、Relu(x)をReluの出力とする。Reluは、入力が0以下の場合は0を、0より大きい場合はそのままの値を出力する関数である。計算が単純であり、計算のコストが小さいという利点がある。入力が0であるとき不連続となり微分は定義できない、という特徴がある。

$`Relu(x)=max(0, x)`$

図は、Reluの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

##### Sigmoid
Sigmoidは、活性化関数の１つである。式は、Sigmoidの関数を示している。xをSigmoidへの入力、Sigmoid(x)をSigmoidの出力とする。Sigmoidは、入力値を0から1の範囲にする関数である。この性質により、確率的な解釈が可能となる。入力値が大きい、または、小さい場合、勾配がほぼゼロになり、学習が進みにくくなる、という問題がある。この問題は、勾配消失問題として知られている。

$`Sigmoid(x)=\frac{1}{1+e^{-x}}`$

図は、Sigmoidの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

##### Softmax
Softmaxは、活性化関数の1つである。式は、Softmaxの関数を示している。ベクトル$`z(z_{1},...z_{K})`$をSoftmaxへの入力、Softmax(z_{i})をSoftmaxの出力とする。多クラス分類の問題において用いられる活性化関数である。Softmaxは入力ベクトルを各クラスに対する確率として正規化する。出力は、それぞれ0から1の間の値を取り、すべての出力の合計は1となる。Softmaxは、Sigmoidの多次元拡張であると言える。

図は、Softmaxの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

$`\mathrm{Softmax}(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}`$

#### 全結合層
全結合層(Fully Connected Layer)は、すべてのノードが接続している層である。受け取った特徴を組み合わせてより抽象的な概念を学習し、最終的な判断や出力を行う役割を持つ。畳み込み層などで抽出された特徴を最終的に分類するために、この全結合層が使われる。
図は、全結合層の接続の構造を示している。

#### Dropout
Dropoutは、正則化のための手法の1つである。Srivastavaらによって、2014年に提案された。正則化とは、モデルが過学習を防ぐための手法である。過学習とは、モデルが訓練データに過度に適合し、未知のデータに対する汎化性能が低下してしまう状態を指す。


Dropoutは、学習の過程でネットワークから、ランダムに、ノードをその接続ごと取り除く作業のことを指す。Dropoutは、隠れ層だけでなく入力層のノードにも適用されるが、出力層には適用されない。学習時には一部のノードが取り除かれるが、推論時には全てのノードを使用する。その際、出力を調整し学習時の挙動と整合性のあるようにする。

### VGGNet
VGGNetは、代表的な畳み込みニューラルネットワークの1つである。オックスフォード大学のVisual Geometry Groupによって、2014年に提案された。VGGNetは、16層(畳み込み層13層と全結合層3層)で構成されるVGG16と、19層(畳み込み層16層と全結合層3層)であるVGG19がある。VGGNetでは、3×3のフィルタを用いる畳み込み層を一貫して用い、層を深くすることによって、抽象的な特徴を捉えるようになることが、図られている。VGGが提案される以前に主流となっていたAlexnetでは、複数の複数の異なる大きさのフィルタを用いていた。

まず、VGG16について説明する。VGG16は、畳み込み層13層と全結合層3層で構成されている。パラメータ数は、約1億3800万である。図は、VGG16のアーキテクチャを示している。層が比較的浅いため、学習速度は速いが、精度が若干低くなる。比較的処理能力を多く必要としないので、精度と計算効率のバランスをとる必要がある場合は、VGG16を選択するべきであると、考えられる。

まず、VGG19について説明する。VGG19は、畳み込み層16層と全結合層3層で構成されている。パラメータ数は、約1億4400万である。図は、VGG19のアーキテクチャを示している。層が比較的深いため、学習速度は遅いが、精度が若干高くなる。比較的多くの処理能力が必要になるので、十分な計算リソースがある場合は、VGG19を選択するべきであると、考えられる。

### スタイル転送技術
スタイル転送とは、スタイルを示している画像とコンテンツを示している画像の2枚から、コンテンツを維持したままスタイルをコンテンツ画像に適用した、新しい画像を生成する技術である。
ニューラルスタイル転送が提案される以前、スタイル転送の手法として、image quiltingによる手法などが提案されていた。その後、ニューラルスタイル転送によって、畳み込みニューラルネットワークから得られる特徴を用いることができるようになった。その後、敵対的生成ネットワーク(GAN, Generative Adversarial Network)を用いるスタイル転送の手法が、提案されている。GANとは、generatorとdiscriminatorの2つから構成されており、generatorが本物らしい画像を生成し、discriminatorがそれを本物か偽物か判別するという対立的な学習を通じて、生成の質を高めていくモデルである。

#### ニューラルスタイル転送
ニューラルスタイル転送(Neural Style Transfer, NST)とは、畳み込みニューラルネットワークが内部で形成する特徴表現を用いて、コンテンツ画像がもつ情報とスタイル画像がもつ情報を同時に最適化することによって、新たな画像を生成する手法である。ニューラルスタイル転送には、記述的な手法と生成的な手法がある。記述的手法とは、ノイズ画像のピクセルを反復的に更新する手法を指し、生成的手法は、スタイルを事前学習されたモデルを用いて、1回の順伝播で行う手法を指す。Leon A. Gatysによるニューラルスタイル転送の手法と、Justin Johnsonによるニューラルスタイル転送の手法について、説明する。

##### Leon A. Gatysによるニューラルスタイル転送の手法
Leon A. Gatysらによって、ニューラルスタイル転送は2015年にはじめて提案された。その後、2016年に、査読付き会議で提案された。この手法は、記述的手法である。

図は、Leon A. Gatysらによって提案されたニューラル転送のアーキテクチャを示している。

まずコンテンツ画像とスタイル画像を、それぞれ、事前学習された畳み込みニューラルネットワークに入力し、特徴マップを抽出する。特徴マップは$`F^{l}_{ij}`$と表す。lは層の位置を、iとjはフィルタの位置を表す。これらの特徴は、層ごとに捉えている情報が異なる点に特徴があり、浅い層ではエッジや色のような低次特徴、深い層では物体の構造や形状のような高次特徴が得られる。この性質を利用することによって、コンテンツとスタイルの役割を明確に分担させることが可能となる。

スタイルの特徴は、式のように、グラム行列を用いる。フィルターとフィルターのベクトル同士の内積で、計算される。グラム行列を用いることによって、位置に依存せず、様々なスケールて情報を捉えることができる。$`N_{l}`$は層lにおけるフィルタの数を表す。$`G_{l}`$は式の通り、行と列の数は、それぞれフィルタの数と等しい。

$`G^{l}_{ij} = \Sigma_{k} F^{l}_{ik}F^{l}_{jk}`$

$`G^{l}\in R^{N_{l}☓N_{l}}`$

コンテンツの損失は式のように、スタイルの損失は式にように、それぞれ計算する。$`E_{l}`$は、は層lにおけるスタイルの誤差（スタイル損失）を表す。そして、式のように、損失を合計する。$`\alpha`$と$`\beta`$は、スタイルとコンテンツの重みを表す。


$`L_{content} = \frac{1}{2}\Sigma_{i,j}(F_{ij}^{l}-P_{ij}^{l})^{2}`$

$`E_{l} = \frac{1}{4M_{l}^{2}M_{l}^{2}}\Sigma_{i,j} (G_{ij}^{l}-A_{ij}^{l})^{2}`$

$`L_{style} = \Sigma^{L}_{l=0}w_{l}E_{l}`$

$`L_{total} = \alpha L_{content} + \beta L_{style}`$

初期画像から、損失が小さくなるように、勾配の計算をする。そして、画像を更新する。

##### Justin Johnsonによるニューラルスタイル転送の手法
Leon A. Gatysによるニューラルスタイル転送の手法が提案されたのち、2016年に、Justin Johnsonらによって、一回の順伝播によってスタイル転送を行うことができる手法が提案された。この手法では、この手法では、一度生成用のネットワークを学習すると、記述的手法に比べて、画像の生成に要する時間が短い。



図は、Justin Johnsonらによって提案されたニューラル転送のアーキテクチャを示している。

### ドメイン適応
学習時のデータと推論時のデータのドメインが異なる際に、一般に、推論の精度が低下することが知られている。ドメインとは、データの特性が異なる環境や条件を指す。特に、本研究でのドメインとは、画像の種類、例えば自然画像、手書きのイラスト、などのように、異なるカテゴリに分類される画像を指す。ドメイン適応とは、ドメインが異なる際に、精度を向上させる問題のことを指す。

ドメイン適応の問題設定として、教師ありドメイン適応、半教師ありドメイン適応、教師なしドメイン適応の3つが挙げられる。教師ありドメイン適応とは、学習時に用いるソースドメインだけでなく、推論対象となるターゲットドメインについても、ラベル付きデータが利用可能な状況を想定した状況である。半教師ありドメイン適応では、ソースドメインにはラベル付きデータが存在するが、ターゲットドメインには一部のラベル付きデータと多数のラベルなしデータが存在する状況を想定した状況である。教師なしドメイン適応とは、ソースドメインにはラベル付きデータが存在する一方で、ターゲットドメインにはラベルなしデータのみが存在する状況を想定した状況である。

ドメイン適応のための手法として、

### データ拡張
データ拡張とは、訓練データが少ない際に、データを増やすことによって、モデルの精度を向上することをすることを目指す手法のことである。ここでは、データ拡張の手法は、幾何学変換と測光変換の2つのカテゴリーの分類される。幾何学変換とは、サイズの変更や回転など、元の画像の形状を変更する手法を指す。一方、測光変換とは、彩度の調整やグレースケール処理など、画像のRGBを変更する手法を指す。更に、近年では、生成的な手法によって画像を増やす手法も提案されている。

データ拡張の問題点として、不適切な方法で行うと、モデルの精度が低下してしまう、という点がある。例えば、自動車のデータセットに対して上下を反転することによるデータ拡張を行うと、実際の走行環境ではほとんど観測されない「逆さまの自動車」という不自然なデータが学習に含まれてしまい、モデルが本来学習すべき特徴と無関係なパターンを覚えてしまう可能性がある。その結果、現実の画像に対する識別性能が低下してしまう。

### ニューラルスタイル転送を用いたデータ拡張による精度向上
先行研究として、ニューラルスタイル転送を用いてデータ拡張を行い、モデルの精度向上を図った、STaDA(Style Transfer as Data Augmentation)という研究がある。この研究では、1回の順伝播で画像を作成することができる、生成型のスタイル転送を用いている。

この研究では、以下に示している７枚の写真を、スタイル画像として、利用している。もとのデータセットに対して、スタイル画像を用いた画像生成を行うことによって、データセットの画像数をに2倍に拡張している。画像は、101個のクラスで構成されている、Caltech 101というラベル付き画像データセットを用いている。

この研究では、スタイル画像のうち、Snowを用いたスタイル転送によって生成した画像を用いることによって、精度が向上することが示された。しかし、Your nameなど、一部のスタイル画像における例では、精度の低下が示された。スタイル画像によって、精度が向上する場合と低下する場合があることが、この研究の結果からわかる。

また、この研究では、伝統的なデータ拡張手法として、FlippingとRotationを使用した実験も行っている。Flippingとは、左右を反転させた画像を生成する操作である。Rotationとは、上下を反転させた画像を生成する操作である。この研究では、Flipping単体では精度は低下するが、WaveとFlippingを組み合わせた手法では精度が向上している。データ拡張手法の組み合わせ方によって、精度に違いがあることが、この研究の結果からわかる。

### 問題点hogehoge(adapt style)

本研究では、ニューラルスタイル転送を用いて

## 提案手法
本章では、提案手法と、

### 基本方針
本研究では、ニューラルスタイル転送を用いて、スタイル画像のドメインの画像を生成する。生成した画像を用いて、画像分類の畳み込みニューラルネットワークの学習を行う。そして、画像分類の精度の変化

### データ拡張手法
本研究では、データ拡張手法として、
### 提案手法から期待する効果

## 実験
### 実験の目的

### データセット
本節では、本研究で用いる画像分類におけるドメイン適応の評価のために用いるPACSデータセット、畳み込みニューラルネットワークの事前学習のためにImageNetの説明を行う。

#### PACSデータセット
本研究では、ドメイン適応の精度を評価するために、4つのドメインで構成されている、PACSデータセットを用いる。Photoのドメインの画像は多くあり、それ以外のドメインの画像は少数のみある、という状況における課題を、今回の研究では扱う。PACSは、Photo Art Cartoon Sketchの略であり、4つのドメインで構成されている、ラベル付き画像データセットである。Photoは1670枚、Artは2048枚、Cartoonは2344枚、Sketchは3929枚で構成されている。各ドメインは、dog、elephant、giraffe、guitar、horse、house、personの、7つのクラスがある。本研究では、各ドメインの画像を三対一の割合で分割し、それぞれを学習用と評価用として利用する。

図は、各ドメインの、クラス毎の画像の例である。

表は、各ドメインの、クラス毎の画像の数を示している。

#### ImageNet
また、VGGNetの事前学習を行うためにImageNetを用いる。ImageNetは、Fei-Fei Liらによって2009年に発表されたラベル付き画像データセットである。1000クラスで構成され、100万枚を超える様々な画像が含まれている。ImageNetによる学習によって獲得する特徴表現は、画像を入力とする多様なタスクに対して、普遍的に有効であることがわかっているので、転移学習に活用されている。

図は、ImageNetに含まれている画像データの例である。


### 実験環境
実験環境として、GPUを搭載している計算機を用いる。図は、実験で実際に用いる計算機のGPU、CPU、メモリ、OSを表す。GPUは、並列の数値計算を高速に行うことができる。ディープラーニングでは並列の計算を多く行うため、GPUを用いることによって、高速化を達成することができる。

ニューラルスタイル転送と画像分類のCNNは、Pythonを用いた。また、ニューラルネットワークの構築と学習用のライブラリとして、TensorflowとKerasを用いて実装を行った。Pythonはバージョン3.12.2、Tensorflowはバージョン2.18.1、Kerasはバージョン3.6.0を用いた。

また、ニューラルスタイル転送と画像分類のCNNにおいて、事前にImagenetを用いて事前学習を行っているVGGNetの重みは、Kerasによって提供されているものを用いる。入手は、VGG16およびVGG19において、以下のコードを用いてそれぞれ行った。modelという変数に、VGGのmodelが代入される。

VGG16
```
import keras
model = keras.applications.VGG16(
    weights="imagenet",
    include_top=False,
)
```

VGG19
```
import keras
model = keras.applications.VGG19(
    weights="imagenet",
    include_top=False,
)
```


本研究で画像分類を行うCNNは、VGG16を基盤とし、、ImageNetで事前学習された重みを用いた畳み込み層部分を特徴抽出器として利用し、そのパラメータを固定した上で、新たに全結合層および分類層を付加したものとする。図は実際のネットワークの構成を示している。

ハイパーパラメータは、表に示したものを用いる。

### 実験1: ベースとなるクラス分類の精度
#### 目的
#### 方法
#### 結果
#### 考察
### 実験2: スタイル画像を複数にした場合の精度
#### 目的
#### 方法
#### 結果
#### 考察
### 実験3: 初期画像別の精度
#### 目的
#### 方法
#### 結果
#### 考察
### 実験4: 古典的手法の組み合わせの精度
#### 目的
#### 方法
#### 結果
#### 考察


### 考察？

## 結論
### 提案手法による成果
### 今後の課題
今後の課題として、まず、画像のクラス分類以外のタスクにおける精度の変化を調べる必要があると、考える。例えば、画像のピクセルに対してラベル付けを行うセマンティックセグメンテーションや、物体の位置と範囲を推論する物体検出が挙げられる。

スタイル転送を用いることによって、ターゲットとするドメインのデータ量の不足を補うことができる可能性が考えられる具体的な例として、医療診断とリモートセンシングが挙げられると、考える。
医療診断では、例えば
リモートセンシングでは、例えば

---
---


# メモ

- https://www.ite.or.jp/contents/keywords/2303keyword.pdf
- https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
- https://keras.io/api/applications/vgg/
- https://www.ibm.com/jp-ja/think/topics/data-augmentation
- https://www.jstage.jst.go.jp/article/iieej/37/6/37_6_815/_pdf/-char/ja
- https://people.eecs.berkeley.edu/~efros/research/quilting/quilting.pdf
- https://speakerdeck.com/ringa_hyj/shen-ceng-xue-xi-wotukatutahua-xiang-sutairubian-huan-falsehua-tojin-madefalseli-shi?slide=2
- https://medium.com/@sandhrabijoy/vgg16-vs-vgg19-a-detailed-comparison-of-the-popular-cnn-architectures-cae5ba404352
- https://zenn.dev/monchy1017/articles/0c725f1d981881
