# 卒論

---

## はじめに

### 研究背景
画像のクラス分類の精度は、畳み込みニューラルネットワーク(convolutional neural network, CNN)によって、飛躍的に向上してきた。畳み込みニューラルネットワークによって飛躍的な精度の向上を達成した理由として、畳み込み層と呼ばれる特徴を抽出する層が局所的なパターンを捉え、それらを積み重ねることでより抽象度の高い特徴を抽出できるようになった点が挙げられる。

しかし、畳み込みニューラルネットワークを用いる際に発生する問題点として、トレーニング用のデータ(トレーニングデータ)とテスト用のデータ(テストデータ)でのドメインの違いによる精度の低下が挙げられる。ドメインとは、データの特性や条件を指す。例えば、自然画像のデータセットで学習されたモデルでは、手書きイラストの画像に対しては、精度が低下してしまうことが考えられる。

このような問題に対応する手法は、ドメイン適応として知られている。ドメイン適応では、トレーニングデータのドメインとテストデータのドメイン間に存在する特徴表現の分布の差を小さくすることが、重要であると考えられる。

本研究では、画像のクラス分類におけるドメイン適応に取り組む。特に、トレーニングデータのドメインと比較して、テストデータのドメインのラベル付き画像が少数しか存在しない状況における問題を扱う。ドメイン適応の手法として、トレーニングデータをテストデータのドメインに、Leon A. Gatysらによって提案されたニューラルスタイル転送(Neural Style Transfer, NST)を用いてスタイル転送を行うことによって、テストデータのドメインに近いスタイルの特徴を持つ画像を生成する。ニューラルスタイル転送では、コンテンツを示す画像(コンテンツ画像)とスタイルを示す画像(スタイル画像)を1枚ずつ用いることによって、コンテンツ画像の構造を保持しつつ、スタイル画像の特徴を反映した画像を生成することができる。
生成された画像を用いて、画像のクラス分類を行う畳み込みニューラルネットワークの学習を行う。そして、提案手法による精度の変化を測定する。

ニューラルスタイル転送を行う際に、テストデータのドメインを示すスタイル画像は1枚に限定されず、複数枚用いることができる場合も想定される。そこで、スタイル画像を複数枚用いることができるように、損失関数を拡張する手法を提案する。また、ニューラルスタイル転送では、勾配計算の際に、同一の損失関数を必ずしも用いる必要はないと考えられる。そこで、損失関数を切り替えながら画像を更新する手法を提案する。さらに、画像の生成時における初期画像として、コンテンツ画像、スタイル画像、およびホワイトノイズ画像の3種類を用い、それぞれがクラス分類の精度に与える影響について評価を行う。加えて、古典的なデータ拡張を組み合わせることがクラス分類の精度に与える影響についても評価を行う。



### 本論文の構成
本論文の構成は、以下のとおりである。
本章では、本研究を行うに至った問題意識と、本研究の提案する手法について述べる。また、本論文の構成について述べる。
第2章では、関連研究として、畳み込みニューラルネットワークの概要と、本研究で用いるVGGNetの構造と特徴について述べる。また、スタイル転送、ドメイン適応およびデータ拡張に関する既存の研究を整理し、ニューラルスタイル転送をデータ拡張に応用した研究事例について述べる。
第3章では、本研究で提案するニューラルスタイル転送を活用するデータ生成の手法について述べる。また、提案する手法によって期待する成果について、評価を行う手法も含めて述べる。
第4章では、本研究で行う実験の具体的な方法と、実験の結果と考察について述べる。
第5章では、結論として、本研究の実験の結果から導かれる考察と今後の課題について述べる。

## 関連研究
本章では、本研究で利用する畳み込みニューラルネットワークの概要とその特徴を示し、本研究で実際に用いるVGGNetの構造と特徴について説明する。また、スタイル転送、ドメイン適応とデータ拡張について、基本的な考え方と、既存の研究を説明する。さらに、ニューラルスタイル転送をデータ拡張に用いた事例を紹介する。そして、本研究の目的について述べる。

### 畳み込みニューラルネットワーク
畳み込みニューラルネットワーク(convolutional neural network, CNN)とは、畳み込み層を用いているニューラルネットワークを指し、画像認識の分野でよく用いられている。畳み込みニューラルネットワークの特徴として、畳み込み層が画像データの一部分ずつにフィルタを適用し、データの局所的なパターンを検出することができることが、挙げられる。

以降では、畳み込みニューラルネットワークの構成要素である、畳み込み層(Convolutional Layer)、プーリング層(Pooling Layer)、活性化関数(Activation Function)および全結合層(Fully Connected Layer)、また、畳み込みニューラルネットワークの学習時に一般的に導入されるDropoutについて、それぞれ説明する。

#### 畳み込み層
畳み込み層(Convolutional Layer)は、入力画像の局所的な特徴を抽出するために機能する。畳み込み層では、フィルタ(あるいはカーネル)と呼ばれる小さな重み行列を、位置を変えながら適用し、そのたびに、対応領域との積和演算を行う。この演算は、畳み込み演算と呼ばれる。図は、畳み込み演算を示している。

畳み込み演算は、ストライドとよばれる移動幅に従って、フィルタを入力画像上で一定間隔ずつスライドさせながら畳み込み演算を行う。以下の図は、ストライドが1の場合と2の場合の畳み込み演算を示している。


畳み込み演算を行う際、入力データの周囲に新たなピクセルを追加する、パディングという作業が行われる。パディングによって、フィルタを入力データの端まで適用できるようになり、出力の空間的なサイズが減少することを、抑制することができる。パディングのうち、周囲のデータをゼロで埋める手法は、ゼロパディングと呼ばれる。

図は、ゼロパディングの流れを示している。

#### プーリング層
プーリング層(Pooling Layer)は、畳み込み層において抽出された特徴マップを圧縮する層である。計算量を削減しつつ、位置ずれに対する頑健性を高める役割がある。プーリング層では、特徴マップを小さな領域に分割し、それぞれの領域から代表値を出力することによって、重要な特徴を保ちながら情報を圧縮することができる。

主なプーリング層の操作として、最大値プーリング(Max Pooling)や平均値プーリング(Average Pooling)がある。最大値プーリングは、各領域の中で最も強く反応した値を選び出す操作であり、特徴の存在そのものを強調する働きがある。平均値プーリングは、値を均等に扱う操作であり、領域全体の情報をなだらかにまとめるため、過度に尖った反応を抑えながら特徴を集約する傾向がある。

図は最大値プーリングの操作を示している。また、図は平均値プーリングの操作を示している。

#### 活性化関数
活性化関数(Activation Function)は、ニューラルネットワークにおいて各ノードが入力信号を受け取った際に、それをどのような形で出力に変換するかを決定する役割を持つ。非線形な活性化関数を導入することによって、ニューラルネットワークは非線形な関係を学習することが可能となり、複雑なデータのパターンを表現できるようになる。

もし非線形な活性化関数を用いない場合、ネットワーク全体は単純な線形変換の連続に過ぎず、層を重ねても表現力は線形モデルと同等にとどまる。そのため、複雑なパターンや特徴を捉えることができず、高度な分類や認識タスクにおいて十分な性能を発揮できない。

ここでは、活性化関数であるReLU、Sigmoid、Softmaxについて説明する。

##### ReLU
ReLUは、活性化関数の1つである。ランプ関数とも呼ばれる。式は、ReLUの関数を示している。xをReLUへの入力、ReLU(x)をReLUの出力とする。ReLUは、入力が0以下の場合は0を、0より大きい場合はそのままの値を出力する関数である。計算が単純であり、計算のコストが小さいという利点がある。入力が0であるとき、不連続となり微分は定義できないという特徴がある。

$`ReLU(x)=max(0, x)`$

図は、ReLUの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

##### Sigmoid
Sigmoidは、活性化関数の1つである。式は、Sigmoidの関数を示している。xをSigmoidへの入力、Sigmoid(x)をSigmoidの出力とする。Sigmoidは、入力値を0から1の範囲へ変換する関数である。この性質により、出力を確率的に解釈することが可能となる。入力値が極端に大きい、または、小さい場合、勾配がほぼゼロになり、学習が進みにくくなるという問題がある。この問題は、勾配消失問題として知られている。

$`Sigmoid(x)=\frac{1}{1+e^{-x}}`$

図は、Sigmoidの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

##### Softmax
Softmaxは、活性化関数の1つである。式は、Softmaxの関数を示している。ベクトル$`z(z_{1},...z_{K})`$をSoftmaxへの入力、$`Softmax(z)`$をSoftmaxの出力とする。多クラス分類の問題において用いられる活性化関数である。出力は、それぞれ0から1の間の値を取り、すべての出力の合計は1となる。そのため、入力ベクトルを各クラスに対する確率として正規化していると言える。また、Softmaxは、Sigmoidの多次元拡張であると言える。

図は、Softmaxの関数の形状を示している。x軸は関数への入力、y軸は関数の出力を示している。

$`\mathrm{Softmax_{i}(z)=\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}}`$

#### 全結合層
全結合層(Fully Connected Layer)は、すべてのノードが接続している層である。受け取った特徴を組み合わせてより抽象的な概念を学習し、最終的な判断や出力を行う役割を持つ。畳み込み層などで抽出された特徴を最終的なクラス分類に使うために、この全結合層が用いられる。

図は、全結合層の接続の構造を示している。

#### Dropout
Dropoutは、正則化のための手法の1つである。Srivastavaらによって、2014年に提案された。正則化とは、モデルが過学習を防ぐための手法である。過学習とは、モデルが訓練データに過度に適合し、未知のデータに対する汎化性能が低下してしまう状態を指す。

Dropoutは、学習の過程でネットワークから、ランダムに、ノードをその接続ごと取り除く作業のことを指す。Dropoutは、隠れ層だけでなく入力層のノードにも適用されるが、出力層には適用されない。学習時には一部のノードが取り除かれるが、推論時には全てのノードを使用する。その際、出力を調整し学習時の挙動と整合性のあるようにする。

### VGGNet
VGGNetは、代表的な畳み込みニューラルネットワークの1つである。オックスフォード大学のVisual Geometry Groupによって、2014年に提案された。VGGNetには、16層(畳み込み層13層と全結合層3層)で構成されるVGG16と、19層(畳み込み層16層と全結合層3層)で構成されるVGG19がある。VGGNetでは、3×3のフィルタを使う畳み込み層を一貫して用い、層を深くすることによって、抽象的な特徴を捉えるようになることが図られている。VGGが提案される以前に主流となっていたAlexNetでは、複数の異なる大きさのフィルタを用いている。

まず、VGG16について説明する。VGG16は、畳み込み層13層と全結合層3層で構成されている。パラメータ数は約1億3800万である。図は、VGG16のアーキテクチャを示している。層の深さが比較的浅いため、一般に学習速度は速い一方で、精度はやや低くなる傾向がある。処理能力を比較的多く必要としないため、精度と計算効率のバランスをとる必要がある場合は、VGG16を選択するべきであると、考えられる。

次に、VGG19について説明する。VGG19は、畳み込み層16層と全結合層3層で構成されている。パラメータ数は約1億4400万である。図は、VGG19のアーキテクチャを示している。層が比較的深いため、一般に学習速度は遅いが、精度がやや高くなる傾向がある。比較的多くの処理能力が必要になるので、十分な計算リソースがある場合は、VGG19を選択するべきであると、考えられる。

### スタイル転送
スタイル転送とは、スタイルを示している画像(スタイル画像)とコンテンツを示している画像(コンテンツ画像)から、コンテンツ画像の内部構造を維持したままスタイル画像の情報を反映した、新しい画像を生成する技術である。
ニューラルスタイル転送が提案される以前、スタイル転送の手法として、Image Quiltingという手法などが提案されていた。Image Quiltingとは、画像を小さく分割し、それをつなぎ合わせることによって、全体として自然な見た目を保ったまま新たな画像を生成する手法である。その後、ニューラルスタイル転送という手法が提案され、畳み込みニューラルネットワークから得られる特徴を用いることができるようになった。さらに、敵対的生成ネットワーク(GAN, Generative Adversarial Network)を用いるスタイル転送の手法が、提案されている。GANとは、生成器と判別器の2つから構成されており、生成器が本物らしい画像を生成し、判別器がそれを本物か偽物か判別するという対立的な学習を通じて、生成の質を高めていくモデルである。GANを用いたスタイル転送では、判別器のフィードバックを通じて、生成画像がより自然で一貫したスタイル分布を持つよう学習が進められる。

ここでは、本研究でスタイル転送に用いる、ニューラルスタイル転送について説明する。

#### ニューラルスタイル転送
ニューラルスタイル転送（Neural Style Transfer, NST）とは、畳み込みニューラルネットワークによって得られる特徴表現を用い、コンテンツ画像の内容とスタイル画像の特徴を同時に反映するよう最適化を行うことで、新しい画像を生成する手法である。ニューラルスタイル転送には、記述的な手法と生成的な手法がある。記述的な手法とは、画像のピクセルを反復的に更新する手法を指し、生成的な手法とは、スタイルを事前学習されたモデルを用いて、画像の生成を1回の順伝播で行う手法を指す。

ここでは、Leon A. Gatysらによって提案されたニューラルスタイル転送の手法と、Justin Johnsonらによって提案されたニューラルスタイル転送の手法について説明する。

##### Leon A. Gatysらによるニューラルスタイル転送の手法
ニューラルスタイル転送は、Leon A. Gatysらによって2015年に初めて提案された。その後、2016年に、査読付き会議で提案された。この手法は、記述的手法である。

図は、Leon A. Gatysらによって提案されたニューラルスタイル転送のアーキテクチャを示している。

この手法では、事前学習済みの畳み込みニューラルネットワークを用い、その重みは固定したまま入力画像のみを最適化する。
画像をネットワークに入力すると、各畳み込み層において特徴マップが得られる。得られた特徴マップは$`F^{l}_{ij}`$と表す。$`l`$は層の位置を、$`i`$と$`j`$はフィルタの位置を表す。畳み込み層では、層ごとに捉えている情報が異なり、浅い層ではエッジや色のような低次特徴、深い層では物体の構造や形状のような高次特徴が得られる。この性質を利用することによって、コンテンツとスタイルの役割を分担させることが可能となる。

スタイルの特徴は、式のように、グラム行列を用いて表現する。グラム行列は、フィルタとフィルタのベクトル同士の内積で計算する。グラム行列を用いることによって、位置に依存せず、様々なスケールで情報を捉えることができる。$`N_{l}`$は層lにおけるフィルタの数を表す。$`G_{l}`$は式で示されているように、行と列の数は、それぞれフィルタの数と等しい。

$`G^{l}_{ij} = \Sigma_{k} F^{l}_{ik}F^{l}_{jk}`$

$`G^{l}\in R^{N_{l}☓N_{l}}`$

コンテンツの損失($`L_{content}`$)は式のように、スタイルの損失($`L_{style}`$)は式のように、それぞれ計算する。
$`P_{ij}^{l}`$はコンテンツ画像から得られた特徴マップを、$`A_{ij}^{l}`$はスタイル画像から得られたグラム行列の成分を表す。
また、$`M_{l}`$は層$`l`$における特徴マップの要素数を、$`w_{l}`$は層$`l`$におけるスタイル損失の重み係数を表す。
$`E_{l}`$は層$`l`$におけるスタイルの誤差（スタイル損失）を表す。そして、式のように、損失を合計する。$`\alpha`$と$`\beta`$は、スタイルとコンテンツの重みを表す。

$`w_{l}`$、$`\alpha`$、$`\beta`$は、ハイパーパラメータであり、事前に設定される値である。

$`L_{content} = \frac{1}{2}\Sigma_{i,j}(F_{ij}^{l}-P_{ij}^{l})^{2}`$

$`E_{l} = \frac{1}{4M_{l}^{2}M_{l}^{2}}\Sigma_{i,j} (G_{ij}^{l}-A_{ij}^{l})^{2}`$

$`L_{style} = \Sigma^{L}_{l=0}w_{l}E_{l}`$

$`L_{total} = \alpha L_{content} + \beta L_{style}`$

初期画像から、合計された損失($`L_{total}`$)が小さくなるように、勾配の計算をする。そして、画像を更新する。
更新の回数を重ねることで、コンテンツ画像の構造を保持しつつ、スタイル画像の特徴を反映した画像が生成される。

##### Justin Johnsonらによるニューラルスタイル転送の手法
Leon A. Gatysらによるニューラルスタイル転送の手法が提案された後、2016年に、Justin Johnsonらによって、生成的手法のスタイル転送の手法が提案された。この手法では、一回の順伝播によってスタイル転送を行うことができる。この手法では、画像生成のためのネットワークを一度学習することで、Leon A. Gatysらによって提案された手法よりも、高速にスタイル転送された画像を生成することができる。

図は、Justin Johnsonらによって提案されたニューラルスタイル転送のアーキテクチャを示している。

まず、この手法では、図ではImage Transform Netと示されている、画像を実際に生成するネットワークによって、入力画像から出力画像を生成する。そして、Image Transform Netによって生成された画像、スタイル画像とコンテンツ画像の3つの画像をそれぞれ畳み込みニューラルネットワークに入力し、特徴量を抽出する。そして、コンテンツの誤差とスタイルの誤差を、Leon A. Gatysらによるニューラルスタイル転送の手法と同じように、計算する。そして、その誤差の情報を用いて、Image Transform Netの重みを更新する。これを繰り返すことによって、特定のスタイルに対して最適化されたImage Transform Netを得ることができる。学習後は任意の画像を入力すると、高速に、スタイル転送された画像を生成できるようになる。

### ドメイン適応
学習時と推論時のデータのドメインが異なる際に、一般に、推論の精度が低下することが知られている。ドメインとは、データの特性や条件を指す。特に、本研究におけるドメインとは、自然画像、水彩画風のイラスト、漫画風のイラストなどのように、画像の種類を指す。ドメイン適応とは、学習時と推論時のデータのドメインが異なる際に、推論の精度を向上させる問題のことを指す。以下では、学習時に用いるデータのドメインのことをソースドメイン、推論時に用いるデータのドメインのことをターゲットドメインと呼ぶ。

ドメイン適応の問題設定として、教師ありドメイン適応、半教師ありドメイン適応、教師なしドメイン適応の3つが挙げられる。教師ありドメイン適応とは、学習時に用いるソースドメインだけでなく、推論対象となるターゲットドメインについても、ラベル付きデータが利用可能であることを想定した状況である。半教師ありドメイン適応とは、ソースドメインにはラベル付きデータが存在するが、ターゲットドメインでは少数のラベル付きデータのみが利用可能であることを想定した状況である。教師なしドメイン適応とは、ソースドメインにはラベル付きデータが存在する一方で、ターゲットドメインにはラベルなしデータのみが存在することを想定した状況である。

ドメイン適応のための手法として、ソースドメインとターゲットドメインの特徴分布の差異が性能低下の原因であることから、敵対的学習による方法や、統計量を調整する方法、また、生成モデルを用いる方法が提案されてきた。
敵対的学習では、ソースドメインとターゲットドメインを識別しようとする識別器と、それを欺くように特徴を生成する特徴抽出器を同時に学習することで、両ドメインの特徴表現が区別できなくなり、結果としてドメイン間の差異を抑えた特徴空間を獲得することができる。
また、統計量を調整する方法では、ソースドメインとターゲットドメインにおける特徴量の平均や分散などの統計的性質を揃えることで、分布のずれを直接的に低減し、ドメインの違いに依存しない安定した推論性能を実現することができる。
さらに、生成モデルを使ったアプローチでは、ターゲットドメインに似たデータを生成することで、ソースドメインのラベル付きデータをターゲットドメイン向けに変換することができる。


### データ拡張
データ拡張とは、モデルの学習に用いるデータを増やすことによって、モデルの精度を向上させることを目指す手法のことである。一般に、データ拡張の手法は、幾何学変換と測光変換の2つのカテゴリーに分類される。幾何学変換とは、サイズの変更や回転など、元の画像の形状を変更する手法を指す。一方、測光変換とは、彩度の調整やグレースケール処理など、画像のRGBを変更する手法を指す。さらに、近年では、生成的な手法によって画像を増やす手法も提案されている。

データ拡張の問題点として、不適切な方法で行うと、モデルの精度が低下してしまうという点がある。例えば、自動車のデータセットに対して上下を反転することによるデータ拡張を行うと、実際の走行環境ではほとんど観測されない「逆さまの自動車」という不自然なデータが学習に含まれてしまい、モデルが本来学習するべきではない特徴を、学習してしまう可能性がある。その結果、現実の画像に対する識別性能が低下してしまう可能性が、考えられる。

### ニューラルスタイル転送を用いたデータ拡張による精度向上
先行研究として、ニューラルスタイル転送を用いてデータ拡張を行い、モデルの精度向上を図った、STaDA(Style Transfer as Data Augmentation)がある。この研究では、1回の順伝播で画像を作成することができる、生成型のニューラルスタイル転送を用いている。

この研究では、以下に示している8枚の写真を、スタイル画像として利用している。もとのデータセットに対してスタイル画像を用いた画像生成を行うことによって、データセットの画像数を2倍に拡張している。画像は、101個のクラスで構成されているCaltech 101というラベル付き画像データセットを用いている。

この研究では、スタイル画像のうち、Snowを用いたスタイル転送によって生成した画像を用いることによって、精度が向上することが示された。しかし、Your nameなど、一部のスタイル画像における例では、精度の低下が示された。スタイル画像の選択によって、精度が向上する場合と低下する場合があることが、この研究の結果からわかる。

また、この研究では、伝統的なデータ拡張手法であるFlippingとRotationを使用した実験も行っている。Flippingとは、左右を反転させた画像を生成する操作である。Rotationとは、上下を反転させた画像を生成する操作である。この研究では、Flipping単体では精度は低下するが、WaveとFlippingを組み合わせた手法では精度が向上している。データ拡張手法の組み合わせ方によって精度に違いがあることが、この研究の結果からわかる。

### 本研究の目的

本研究では、画像のクラス分類における半教師ありドメイン適応の課題に対し、ニューラルスタイル転送を用いて生成したデータを画像のクラス分類のモデルの学習に用いることによる、分類精度の変化を評価する。

本研究で扱う問題設定では、スタイル画像が1枚に限定されるとは限らず、複数枚存在する状況が想定される。また、ニューラルスタイル転送では、勾配計算の際に、同一の損失関数を必ずしも用いる必要はないと考えられる。そこで本研究の目的は、複数のスタイル画像を用いて生成した画像を学習データとして利用した場合、また、勾配計算の際に損失関数を変化させた場合に、それぞれ分類精度がどのように変化するかを評価することとする。

加えて、ニューラルスタイル転送における初期画像の違いが生成結果に与える影響や、従来の古典的なデータ拡張手法と組み合わせた場合の分類精度の変化も評価する。

<!--
本研究では、画像のクラス分類における、半教師ありドメイン適応の問題に取り組む。半教師ありドメイン適応では、ソースドメインには十分なラベル付きデータが存在する一方で、ターゲットドメインには少数のラベル付きデータしか存在しない。そのため、ターゲットドメインの特徴を十分に学習できず、分類精度が低下するという課題がある。そこで、Leon A. Gatysらが提案したニューラルスタイル転送の手法によって、ソースドメインの画像をターゲットドメインに変換する。

一般に、ニューラルスタイル転送では、スタイル画像は1枚のみ用いる。しかし、本研究で行う問題では、スタイル画像を複数用いることができる状況を考える。このような問題設定において、スタイル画像を複数用いることができるように損失関数を拡張することによって、複数のスタイル画像から得られるスタイル特徴を同時に反映できると、考えられる
また、勾配の計算に用いる損失関数を反復過程の途中で切り替えながら画像を更新する手法が、考えられる。
さらに、Leon A. Gatysらによるニューラルスタイル転送では、任意の画像から更新を繰り返すことによって、画像を生成することができる。
加えて、ニューラルスタイル転送を用いたデータ拡張による精度向上の研究によって、伝統的なデータ拡張手法が、モデルの精度向上に有効である可能性が示されている。

そこで、本研究では、ニューラルスタイル転送によるデータ生成における、損失関数の拡張による複数のスタイル画像の利用、勾配計算のための損失関数の切り替え、また、ニューラルスタイル転送における初期画像の違いが、ドメイン適応の課題における分類精度に与える影響を調べることを目的とする。
さらに、古典的なデータ拡張手法を組み合わせることによる影響も明らかにする。

以上の方針に基づき、本研究では、ニューラルスタイル転送を用いたデータ生成を通じて、半教師ありドメイン適応における学習データの不足という課題に対処する。
-->
## 提案手法
本研究の目的は、ニューラルスタイル転送を用いてデータを生成する際の、スタイル画像を複数用いた場合の精度の変化と、勾配計算時に用いる損失関数を切り替えた場合の精度の変化を、評価することである。さらに、初期画像の違い、古典的なデータ拡張手法の組み合わせが、それぞれ分類精度に与える影響を明らかにすることである。そこで、本章では、その目的を実現するために提案する、ニューラルスタイル転送によるデータ生成の手法について述べる。

### 基本方針
はじめに、ニューラルスタイル転送を用いて、トレーニングデータを、ターゲットドメインにスタイル転送する。
次に、生成された画像を学習データとして用い、畳み込みニューラルネットワークの学習を行う。
そして、得られたモデルを用いて評価を行い、ターゲットドメインにおける分類精度を得る。これらの結果をもとに、提案手法により生成した画像が、分類性能にどのような影響を与えるかを評価する。


### データ拡張手法
Leon A. Gatysらが提案したニューラルスタイル転送では、一般に、スタイル画像を1枚のみ用いる。そこで、本研究では、複数のスタイル画像を用いたスタイル転送を可能とするために、損失関数の拡張を行う方法を提案する。

スタイル損失の拡張は、式のように行う。$`L_{style\_i}`$を、i番目のスタイル画像、$`\gamma_{i}`$をi番目のスタイル画像に対応する重み係数、Iをスタイル画像の数とする。

$`L_{multiple\_styles}=\Sigma_{I} \gamma_{i} L_{style\_i}`$

そして、全体のスタイル損失は、式のようになる。

$`L_{total}= \alpha L_{multiple\_styles} + \beta L_{content}`$　

また、勾配計算の際に、損失関数を毎回同じものを用いる必要はない。そこで、勾配計算に用いる損失関数を切り替える手法を提案する。

書く。


### 提案手法から期待する効果
これまで説明した手法によって生成した画像を、単体で畳み込みニューラルネットワークの学習に用いる場合と、元のトレーニングセットの画像と組み合わせて学習に用いる場合の2通りで使用する。そして、画像のクラス分類タスクを用いて、提案手法の効果を測定する。

提案手法を用いることによって、ターゲットドメインのスタイルを反映した学習用画像を効率的に生成することが可能となることが、期待される。その結果、限られたターゲットドメインのラベル付きデータのみでは十分に学習できなかった特徴を補完することが、期待される。そして、ターゲットドメインに対するクラス分類の精度が向上することが期待される。

## 実験
本章では、実験の目的、実験の基本構成、使用するデータセット、実験環境について説明する。また、本研究で行う5つの実験について、それぞれ説明する。

### 実験の目的
実験では、第3章で提案した複数のスタイル画像を用いたニューラルスタイル転送によるデータ生成手法が、画像のクラス分類の精度向上に対して有効かを確かめる。具体的には、スタイル損失関数の拡張および勾配計算時に損失関数を切り替える手法、ならびに初期画像の違いが生成画像と分類精度に与える影響を考慮した半教師ありドメイン適応手法を実行し、手法の有効性を確認する。
加えて、古典的なデータ拡張手法を併用した場合における分類精度の変化を評価し、半教師ありドメイン適応の性能にどのような影響を与えるかを明らかにする。

### 実験の基本構成
図は、実験の流れを示している。

まず、十分なラベル付きデータが存在するソースドメインの画像と、少数のラベル付きデータのみが存在するターゲットドメインの画像を準備する。本研究では、以下で述べるPACSデータセットを用いる。
次に、ソースドメインの各画像をコンテンツ画像とし、ターゲットドメインの画像をスタイル画像として用いることで、ニューラルスタイル転送を行う。スタイル画像は、各ドメインから、それぞれのクラスでランダムに選択する。
生成された画像は、ソースドメインの元の学習用画像と組み合わせて、新たな学習用データセットを構成する。その後、この拡張されたデータセットを用いて、畳み込みニューラルネットワークによる画像のクラス分類モデルを学習する。
学習が完了したモデルに対しては、ターゲットドメインの評価用データを入力し、クラス分類の精度を算出することで性能を評価する。クラス分類の精度は、Top1精度、すなわち、最も確率の高いクラスが正解クラスである場合に正解であるとみなす指標で、評価を行う。


本研究で行うすべての実験は、この一連の処理の流れを基本構成とする。各実験では、条件の一部の要素のみを変更することで、それぞれ分類精度に与える影響を調べる。

### データセット
本節では、本研究で用いる画像分類におけるドメイン適応の評価のために用いるPACSデータセット、畳み込みニューラルネットワークの事前学習のためにImageNetの説明を行う。

#### PACSデータセット
本研究では、ドメイン適応の精度を評価するために、複数のドメインで構成されているラベル付き画像データセットである、PACSデータセットを用いる。PACSデータセットは、Photo、Art、Cartoon、Sketchの4つのドメインで構成されている。
各ドメインは、dog、elephant、giraffe、guitar、horse、house、personの7つのクラスで構成されている。本研究では、各ドメインの画像を三対一の割合で分割し、それぞれを学習用と評価用として利用する。


表では、各ドメインおよび各クラスごとの、学習用画像の数と評価用画像の数を、それぞれ示している。


本研究の実験では、Photoのドメインをソースドメインとして利用し、それ以外のドメインはターゲットドメインとして用いる。

図は、各ドメインの、クラスごとの画像データの例である。

#### ImageNet
また、VGGNetの事前学習を行うためにImageNetを用いる。ImageNetは、Fei-Fei Liらによって2009年に発表されたラベル付き画像データセットである。1000クラスで構成され、100万枚を超える様々な画像が含まれている。ImageNetによる学習によって獲得する特徴表現は、画像を入力とする多様なタスクに対して普遍的に有効であることがわかっているので、転移学習に活用されている。

図は、ImageNetに含まれている画像データの例である。


### 実験環境
実験環境として、GPUを搭載している計算機を用いる。この計算機では、ニューラルスタイル転送による画像生成と、画像分類のための畳み込みニューラルネットワークの学習および推論の両方を行う。GPUは、並列の数値計算を高速に行うことができる。ディープラーニングでは並列の計算を多く行うため、GPUを用いることによって、高速化を達成することができる。

ニューラルスタイル転送と画像分類の畳み込みニューラルネットワークは、Pythonを用いた。また、ニューラルネットワークの構築と学習用のライブラリとして、TensorFlowとKerasを用いて実装を行った。それぞれ、TensorFlowはバージョン2.18.1、Kerasはバージョン3.6.0を用いた。

なお、実験1において、生成と推論における速度の計測は、以下の図で示した環境で行った。
<!-- GPU、CPU、メモリ、OS 
GPU: GeForce RTX 4090（24GB）
CPU: Intel Core i9-13900K
メモリ: 64GB
OS: Ubuntu 24.04.2 LTS

-->
また、ニューラルスタイル転送と画像分類の畳み込みニューラルネットワークにおいて、事前にImageNetを用いて事前学習を行っているVGGNetの重みは、Kerasによって提供されているものを用いる。
ニューラルスタイル転送における、コンテンツ画像の特徴とスタイル画像の特徴の抽出を行う畳み込みニューラルネットワークは、VGG19を用いる。重みはImageNetによって事前学習されているものを用いる。コンテンツの特徴抽出には、block5_convの畳み込み層を用いる。一方、スタイルの特徴抽出には、block1_conv1、block2_conv1、block3_conv1、block4_conv1、block5_conv1の5つの畳み込み層を用いる。


画像分類を行う畳み込みニューラルネットワークは、VGG16を基盤とし、ImageNetで事前学習された重みを用いた畳み込み層の部分を特徴抽出器として利用し、そのパラメータを固定した上で、新たに全結合層および分類層を付加したものとする。図は実際のネットワークの構成を示している。

本研究で行う実験は、特に後述しない限り、ハイパーパラメータは、表に示したものを用いる。


### 実験1: ベースとなるクラス分類の精度
#### 目的
実験1では、本研究で提案するニューラルスタイル転送を用いたデータ生成手法が、画像のクラス分類精度にどの程度影響を与えるかを確認することを目的とする。具体的には、スタイル転送によって生成した画像を学習データとして用いる場合と、用いない場合とを比較することで、データ生成による効果を明らかにする。また、ニューラルスタイル転送による画像生成に要する時間および、生成されたデータセットを用いた畳み込みニューラルネットワークの学習に要する時間を測定し、本手法の実用性を検証する。

#### 方法
ニューラルスタイル転送を用いて、Photoドメインの画像を、それぞれのクラスに対応するスタイル画像を用いて、スタイル転送を行う。
生成した画像を用いて、畳み込みニューラルネットワークの学習を行う。

1つのドメインあたり1333枚の画像を生成している。本実験では、それに要した合計の時間を測定する。1枚あたりの時間は、合計の時間を1333で割り計算する。

実験1では、スタイル画像は表に示したものを用いる。
#### 結果
ニューラルスタイル転送によって生成した画像を用いずに学習した場合の結果

| ターゲットドメイン | 精度 |
|----|---------|
| art painting |0.465 |
| cartooon |0.285 |
| sketch | 0.374 |

ニューラルスタイル転送によって生成した画像を用いた場合の結果

| ターゲットドメイン | 精度 |
|----|---------|
| art painting |0.474, 0.462, 0.467, 0.472|
| cartooon |0.346, 0.384, 0.380, 0.350|
| sketch |0.421, 0.307, 0.410, 0.407|

また、以下の表は、ニューラルスタイル転送によって画像を生成するのに要した時間を示している。1つのドメインあたり1333枚の画像を生成しており、それに要した合計の時間を示している。また、「1枚あたり」の列は、１つのドメインに要した合計時間を1333で割り、1枚あたりに要した時間の平均を取ったものである。

| 項目 | 生成時間(秒) |
|----|---------|
| 1ドメイン分 | 19192 |
| 1枚あたり | 14 |


<!--
以下の表は、画像認識の精度の結果を示している。


さらに、以下の表は、画像のクラス分類のための畳み込みニューラルネットワークの学習に要した時間を示している。

ニューラルスタイル転送によってデータセットを作成するために、おおよそ1時間程度学習に要することがわかる。そして、画像のクラス分類のための畳み込みニューラルネットワークの学習に要する時間は、およそn分であることがわかる。
-->
#### 考察
<!--
まず、クラス分類の精度の結果から、

また、ニューラルスタイル転送によってデータセットを作成するために要する時間と、画像のクラス分類のための畳み込みニューラルネットワークの学習に要する時間の結果から、本研究の提案手法では殆どの時間が、ニューラルスタイル転送に充てられている。したがって、ニューラルスタイル転送の高速化が進むことによって、本研究の提案手法の実効性が高まると、考える。

-->
### 実験2: スタイル画像を複数にした場合の精度
#### 目的
実験2では、ニューラルスタイル転送においてスタイル画像を複数枚用いた場合に、画像のクラス分類精度がどのように変化するかを明らかにすることを目的とする。単一のスタイル画像を用いる従来の設定と比較し、複数のスタイル画像から得られるスタイル特徴を反映させることが、分類性能をどのように変化させるかを検証する。

#### 方法
各ドメインから、それぞれのクラスで2枚ずつスタイル画像を、ランダムに選択する。
ニューラルスタイル転送を用いて、Photoドメインの画像を、それぞれのクラスに対応する2枚のスタイル画像を用いて、スタイル転送を行う。
複数のスタイル画像を用いたニューラルスタイル転送は、提案手法の章で述べた手法で行う。
この手法で生成した画像を用いて、畳み込みニューラルネットワークの学習を行う。

実験２では、スタイル画像は表に示したものを用いる。
<!--
本研究では、スタイルを示す画像として、表に示した画像を用いた。図は、実際に用いるスタイル画像を示している。
--->
#### 結果
<!--
表はターゲットドメインがart、表はターゲットドメインがcartoon、表はターゲットドメインがsketch、における画像認識の精度の結果をそれぞれ示している。
-->

| ターゲットドメイン | 精度 |
|----|---------|
| art painting |0.465, 0.477|
| cartooon |0.361, 0.374|
| sketch |0.377, 0.359|


#### 考察

### 実験3: 勾配計算時に損失関数を切り替える手法の精度
#### 目的
実験3では、ニューラルスタイル転送の画像を更新する過程において、勾配計算に用いる損失関数を反復の途中で切り替える手法が、生成画像およびクラス分類精度に与える影響を調べることを目的とする。

#### 方法
勾配計算時に損失関数を切り替えるニューラルスタイル転送は、提案手法の章で述べた手法で行う。
この手法によって生成した画像を用いて、畳み込みニューラルネットワークの学習を行う。

実験3では、スタイル画像は表に示したものを用いる。
#### 結果
| ターゲットドメイン | 精度 |
|----|---------|
| art painting |0.484, 0.455|
| cartooon |0.350, 0.401|
| sketch |0.412, 0.317|
#### 考察

### 実験4: 初期画像別の精度
#### 目的
実験4では、ニューラルスタイル転送を用いて画像を生成する際の初期画像によって、精度がどのように変化するかを測定する。ニューラルスタイル転送では、初期画像を更新することによって、画像の生成を行う。ここで、初期画像をコンテンツ画像にする場合、スタイル画像にする場合、ホワイトノイズ画像にする場合のそれぞれによって、生成される画像の質が変わると考えられる。そこで、それぞれ初期画像を使った場合の精度を比較し、最適な初期画像の選定に関する知見を得ることを目指す。

#### 方法
初期画像として、コンテンツ画像、スタイル画像、ホワイトノイズ画像の3種類を用いて、ニューラルステイル転送によって画像を生成する。それぞれ生成した画像を用いて、畳み込みニューラルネットワークの学習を行う。

ホワイトノイズ画像とは、各ピクセルの値がランダムに決定され、空間的な構造や意味的な情報を一切含まない画像のことである。本実験で用いるホワイトノイズ画像は、〇〇。
図は、実際に用いるホワイトノイズ画像を示している。


実験4では、スタイル画像は表に示したものを用いる。
#### 結果

スタイルがデフォルト、の結果

| ターゲットドメイン | 精度 |
|----|---------|
| art painting |0.465, 0.465 |
| cartooon | 0.423, 0.395|
| sketch |0.486, 0.445 |


<!--
表はターゲットドメインがart、表はターゲットドメインがcartoon、表はターゲットドメインがsketch、における画像認識の精度の結果をそれぞれ示している。
-->
#### 考察


### 実験5: 古典的データ拡張手法の組み合わせた場合の精度
#### 目的
実験5では、ニューラルスタイル転送によるデータ生成と、FlippingやRotationといった古典的なデータ拡張手法を組み合わせた場合に、画像のクラス分類精度がどのように変化するかを明らかにすることを目的とする。

#### 方法
ニューラルスタイル転送を用いて画像を生成する。また、画像を左右反転する操作であるFlippinと、画像を上下反転する操作であるRotationによって、画像を生成する。それぞれの生成した画像を組み合わせて、畳み込みニューラルネットワークの学習を行う。
実験5では、スタイル画像は表に示したものを用いる。
#### 結果
<!--
表はターゲットドメインがart、表はターゲットドメインがcartoon、表はターゲットドメインがsketch、における画像認識の精度の結果をそれぞれ示している。
-->
#### 考察


### 5つの実験結果を通しての考察

## 結論
### 提案手法による成果
<!--
本研究の結果から、ニューラルスタイル転送によって画像を生成し、その画像も含めて画像のクラス分類のモデルを学習することによって、精度の向上が期待できることが、示された。
-->
### 今後の課題と活用の展望


今後の課題として、提案手法の実効性を高めるために、より高速に画像を生成することができるスタイル転送技術を用いてデータ生成を行った場合の、クラス分類の精度の評価を行う点が挙げられる。

また、画像のクラス分類以外のタスクにおける精度の変化を調べる必要があると考える。例えば、画像のピクセルに対してラベル付けを行うセマンティックセグメンテーションや、物体の位置と範囲を推論する物体検出が挙げられる。


スタイル転送を用いることによって、ターゲットとするドメインのデータ量の不足を補うことができる可能性が考えられる具体的な例として、医療診断とリモートセンシングが挙げられる。
医療診断では、異なる医療機器間で生じる画像の質感の差異における問題が、考えられる。特に、高価な機器や希少な機器において、データが不足することが考えられる。そこで、広く用いられている機器で取得されたデータをコンテンツ画像として利用しデータを生成することで、学習に十分なデータセットを用意することができると考える。
また、リモートセンシングでは、天候や季節、時間帯によって地表の見た目が異なり、十分なデータセットを用意できないことが考えられる。そこで、データが十分に取得できない条件下で観測された画像に対しても、提案手法を用いることで、別の環境で取得された豊富なデータを活用できると考える。

このように、データがあまり取得できない条件下で観測された画像に対しても、スタイル転送によって生成したデータを利用するすることで、精度の向上が期待される。


## 付録
ImageNetによって事前学習されているVGGのモデルは、以下のコードを用いて入手した。modelという変数に、VGGのmodelが代入される。

VGG16
```
import keras
model = keras.applications.VGG16(
    weights="imagenet",
    include_top=False,
)
```

VGG19
```
import keras
model = keras.applications.VGG19(
    weights="imagenet",
    include_top=False,
)
```

---
---


<!--
# メモ

- https://www.ite.or.jp/contents/keywords/2303keyword.pdf
- https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
- https://keras.io/api/applications/vgg/
- https://www.ibm.com/jp-ja/think/topics/data-augmentation
- https://www.jstage.jst.go.jp/article/iieej/37/6/37_6_815/_pdf/-char/ja
- https://people.eecs.berkeley.edu/~efros/research/quilting/quilting.pdf
- https://speakerdeck.com/ringa_hyj/shen-ceng-xue-xi-wotukatutahua-xiang-sutairubian-huan-falsehua-tojin-madefalseli-shi?slide=2
- https://medium.com/@sandhrabijoy/vgg16-vs-vgg19-a-detailed-comparison-of-the-popular-cnn-architectures-cae5ba404352
- https://zenn.dev/monchy1017/articles/0c725f1d981881

- https://arxiv.org/pdf/2411.07072
---
- https://arxiv.org/pdf/1708.01155
- https://arxiv.org/pdf/1803.01229

---
---

# 表

-->
