# 卒論

---


## はじめに


## 関連研究
本章では、提案手法で利用する畳み込みニューラルネットワークの概要とその特徴を示し、また、本研究で採用する VGGNetの構造と特徴について、説明する。さらに、本研究が取り組むドメイン適応における、関連する既存研究について説明する。

### 畳み込みニューラルネットワーク
畳み込みニューラルネットワーク(convolutional neural network, CNN)とは、畳み込みを用いているニューラルネットワークを指し、画像認識の分野で用いられている。CNNの特徴として、データの局所的なパターンを検出することができる。

以降では、畳み込み層、プーリング層、活性化関数、全結合層およびDropoutについて、それぞれの仕組みを、説明する。

#### 畳み込み層
畳み込み層は、入力画像の局所的な特徴を抽出するために、機能する。

フィルタ(あるいはカーネル)と呼ばれる小さな重み行列を、ストライドとよばれる移動幅に従って、位置を変えながら適用し、そのたびに、対応領域との積和演算を行う。この演算は、畳み込み演算と呼ばれる。

畳み込みを適用する際に、入力データの周囲に新たなピクセルを追加する、パディングという作業が行われる。パディングによって、フィルタが入力データの端まで適用できるようになる。パディングを行うことによって、出力の空間的なサイズが減少することを、抑制することができる。パディングのうち、周囲のデータをゼロで埋める手法は、ゼロパディングと呼ばれる。

#### プーリング層
プーリング層は、畳み込み層において抽出された特徴マップを圧縮する層である。計算量を削減しつつ、位置ずれに対する頑健性を高める役割がある。特徴マップを小さな領域に分割し、各領域から代表値を出力することによって、重要な特徴を保ちながら情報を圧縮する。主なプーリング層の操作として、最大値プーリング(Max Poolig)や平均プーリング(Average Pooling)がある。最大値プーリングは、各領域の中で最も強く反応した値を選び出すため、特徴の存在そのものを強調する働きがある。平均プーリングは、値を均等に扱って平均を求める操作であり、領域全体の情報をなだらかにまとめるため、過度に尖った反応を抑えながら特徴を集約する傾向がある。

#### 活性化関数
活性化関数は、ニューラルネットワークの入力から、各ニューロンが受け取った入力信号をどのように出力するかを決定する役割を持つ。活性化関数を導入することによって、ネットワークは非線形な関係を学習することが可能となり、複雑なデータのパターンを表現できるようになる。非線形な活性化関数を用いない場合、ネットワーク全体は単純な線形変換の連続に過ぎず、層を重ねても表現力は線形モデルと同等にとどまる。そのため、複雑なパターンや特徴を捉えることができず、高度な分類や認識タスクにおいて十分な性能を発揮できない。

##### Relu
Reluは、活性化関数の1つである。ランプ関数とも呼ばれる。入力が0以下の場合は0を、0より大きい場合はそのままの値を出力する関数である。計算が単純であり、計算のコストが小さいという利点がある。入力が0であるとき、不連続となり、微分は定義できない。

##### Sigmoid
Sigmoidは、入力値を0から1の範囲にする関数である。この性質により、確率的な解釈が可能となる。入力値が大きい、または、小さい場合、勾配がほぼゼロになり、学習が進みにくくなる、という問題がある。この問題は、勾配消失問題として知られている。

##### Softmax
Softmaxは、活性化関数の1つである。多クラス分類の問題において用いられる活性化関数である。Softmaxは入力ベクトルを各クラスに対する確率として正規化する。出力は、0から1の間の値を取り、すべての出力の合計は1となる。Softmaxは、Sigmoidの多次元拡張であると言える。

#### 全結合層
全結合層は、すべてのノードが接続している層である。受け取った特徴を組み合わせてより抽象的な概念を学習し、最終的な判断や出力を行う役割を持つ。畳み込み層などで抽出された特徴を最終的に分類するために、この全結合層が使われる。

#### Dropout
Dropoutは、正則化のための手法の1つである。Srivastavaらによって、2014年に提案された。正則化とは、モデルが訓練データに過度に適合する「過学習」を防ぎ、未知のデータに対する汎化能力を高めるための手法である。

Dropoutでは、学習の過程でネットワークから、ランダムに、ノードをその接続ごと取り除く。Dropoutは、隠れ層だけでなく入力層のノードにも適用されるが、出力層には適用されない。学習時には一部のノードが取り除かれるが、推論時には全てのノードを使用する。その際、出力を調整し学習時の挙動と整合性のあるようにする。

### VGGNet
VGGNetは、代表的な畳み込みニューラルネットワークの1つである。オックスフォード大学のVisual Geometry Groupによって、2014年に提案された。VGGNetは、16層(畳み込み層13層と全結合層3層)で構成されるVGG16と、19層(畳み込み層16層と全結合層3層)であるVGG19がある。VGGNetでは、3×3のフィルタを用いる畳み込み層を一貫して用い、層を深くすることによって、抽象的な特徴を捉えるようになることが、図られている。そして、

まず、VGG16について説明する。以下の図が、VGG16のアーキテクチャである。VGG16の特徴として、

まず、VGG19について説明する。以下の図が、VGG19のアーキテクチャである。VGG19の特徴として、

### スタイル転送技術
スタイル転送とは、スタイルを示す画像とコンテンツを示す画像の2枚から、コンテンツを維持したままスタイルをコンテンツ画像に適用した、新しい画像を生成する技術である。

#### ニューラルスタイル転送以前のスタイル転送技術
ニューラルスタイル転送が提案される以前、ノンフォトリアリスティックレンダリング呼ばれる領域が発展していた。ノンフォトリアリスティックレンダリングとは、

#### ニューラルスタイル転送
ニューラルスタイル転送とは、畳み込みニューラルネットワークが内部で形成する特徴表現を用いて、コンテンツ画像がもつ情報とスタイル画像がもつ情報を同時に最適化することによって、新たな画像を生成する手法である。ニューラルスタイル転送には、記述的な手法と生成的な手法がある。記述的手法とは、ノイズ画像のピクセルを反復的に更新する手法を指し、生成的手法は、スタイルを事前学習されたモデルを用いて、1回の順伝播で行う手法を指す。

Leon A. Gatysらによって、ニューラルスタイル転送は2015年にはじめて提案された。その後、2016年に、査読付き会議で提案された。この手法は、記述的手法である。その後、

本研究では、

##### 基本概念
以下の図は、Leon A. Gatysらによって提案されたニューラル転送のアーキテクチャを示す図である。

まずコンテンツ画像とスタイル画像をそれぞれ畳み込みニューラルネットワークに入力し、特徴マップを抽出する。これらの特徴は、層ごとに捉えている情報が異なる点に特徴があり、浅い層ではエッジや色のような低次特徴、深い層では物体の構造や形状のような高次特徴が得られる。この性質を利用することによって、コンテンツとスタイルの役割を明確に分担させることが可能となる。

### ドメイン適応
ドメイン適応とは、

#### ドメイン適応の種類
#### 既存手法

## 提案手法
### 基本方針
### データ拡張手法
### 提案手法から期待する効果

## 実験
### 実験の目的
### 実験環境
### データセット
本研究では、ドメイン適応の精度を評価するために、4つのドメインで構成されている、PACSデータセットを用いる。また、VGGNetの事前学習を行うためにImageNetを用いる。

#### PACSデータセット
PACSは、Photo Art Cartoon Sketchの略であり、4つのドメインで構成されている、ラベル付き画像データセットである。Photoは1670枚、Artは2048枚、Cartoonは2344枚、Sketchは3929枚で構成されている。各ドメインは、dog、elephant、giraffe、guitar、horse、house、personの、7つのクラスがある。本研究では、各ドメインの画像を三対一の割合で分割し、それぞれを学習用と評価用として利用する。

#### ImageNet
ImageNetは、Fei-Fei Liらによって発表されたラベル付き画像データセットである。

### 実験1: 
#### 目的
#### 方法
#### 結果
#### 考察
### 実験2: 
#### 目的
#### 方法
#### 結果
#### 考察
### 実験3: 
#### 目的
#### 方法
#### 結果
#### 考察
### 実験4: 
#### 目的
#### 方法
#### 結果
#### 考察

## 結論
### 提案手法による成果
### 課題


---
---


# メモ

- https://www.ite.or.jp/contents/keywords/2303keyword.pdf
- https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
