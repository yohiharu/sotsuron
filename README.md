# 卒論

---


## はじめに


## 関連研究
本章では、提案手法で利用する畳み込みニューラルネットワークの概要とその特徴を示し、また、本研究で採用する VGG の構造と特徴について、説明する。さらに、本研究が取り組むドメイン適応における、関連する既存研究について説明する。

### 畳み込みニューラルネットワーク
畳み込みニューラルネットワーク(convolutional neural network, CNN)とは、畳み込みを用いているニューラルネットワークを指し、画像認識の分野で用いられている。CNNの特徴として、データの局所的なパターンを検出することができる。

以降では、畳み込み層、プーリング層、活性化関数、全結合層およびDropoutについて、それぞれの仕組みを、説明する。

#### 畳み込み層
畳み込み層は、入力画像の局所的な特徴を抽出するために、機能する。

フィルタ(あるいはカーネル)と呼ばれる小さな重み行列を、ストライドとよばれる移動幅に従って、位置を変えながら適用し、そのたびに、対応領域との積和演算を行う。この演算は、畳み込み演算と呼ばれる。

畳み込みを適用する際に、入力データの周囲に新たなピクセルを追加する、パディングという作業が行われる。パディングによって、フィルタが入力データの端まで適用できるようになる。パディングを行うことによって、出力の空間的なサイズが減少することを、抑制することができる。パディングのうち、周囲のデータをゼロで埋める手法は、ゼロパディングと呼ばれる。

#### プーリング層

#### 活性化関数
##### Relu
Reluは、活性化関数の1つである。ランプ関数とも呼ばれる。計算が単純であり、計算のコストが小さいという利点がある。入力が0であるとき、不連続となり、微分は定義できない。

##### Sigmoid

##### Softmax
Softmaxは、活性化関数の1つである。多クラス分類の問題において、。Softmaxは、Sigmoidを多次元に

#### 全結合層
#### Dropout
Dropoutは、正則化のための手法の1つであり、2014年にSrivastavaらによって提案された。正則化とは、モデルが訓練データに過度に適合する「過学習」を防ぎ、未知のデータに対する汎化能力を高めるための手法である。

Dropoutでは、学習の過程でネットワークから、ランダムに、ノードをその接続ごと取り除く。Dropoutは、隠れ層だけでなく入力層のノードにも適用されるが、出力層には適用されない。学習時には一部のノードが取り除かれるが、推論時には全てのノードを使用する。その際、出力を調整し学習時の挙動と整合性のあるようにする。
### VGG

### ニューラルスタイル転送
#### 基本概念
#### 特徴抽出
##### コンテンツにおける特徴抽出
##### スタイルにおける特徴抽出
#### 損失関数
#### 画像生成

### ドメイン適応
#### ドメイン適応の種類
#### 既存手法

## 提案手法
### 基本方針
### データ拡張手法
### 提案手法から期待する効果

## 実験
### 実験の目的
### 実験環境
### データセット
#### PACSデータセット
#### ImageNet
### 実験1: 
#### 目的
#### 方法
#### 結果
#### 考察
### 実験2: 
#### 目的
#### 方法
#### 結果
#### 考察
### 実験3: 
#### 目的
#### 方法
#### 結果
#### 考察
### 実験4: 
#### 目的
#### 方法
#### 結果
#### 考察

## 結論
### 提案手法による成果
### 課題
